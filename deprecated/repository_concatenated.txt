===== ./autopub.py =====
#!/usr/bin/env python3
# autopub.py - Video processing and publishing system

import os
import csv
import re
import json
import argparse
import requests
from datetime import datetime
from pathlib import Path
import subprocess
import sys

# Get the directory where this script is located
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

# Import local modules
sys.path.append(SCRIPT_DIR)
try:
    from process_video import VideoProcessor
    from selenium.webdriver.chrome.service import Service
except ImportError as e:
    print(f"Error importing required modules: {e}")
    print("Make sure all required packages are installed.")
    sys.exit(1)

# Paths for the folders and files
logs_folder_path = os.path.join(SCRIPT_DIR, 'logs')
home_dir = os.path.expanduser("~")
autopublish_folder_path = os.path.join(home_dir, 'AutoPublishDATA', 'AutoPublish')
videos_db_path = os.path.join(SCRIPT_DIR, 'videos_db.csv')
processed_path = os.path.join(SCRIPT_DIR, 'processed.csv')
transcription_path = os.path.join(home_dir, 'AutoPublishDATA', 'transcription_data')

# Ensure the logs, videos, and database files exist
os.makedirs(logs_folder_path, exist_ok=True)
os.makedirs(autopublish_folder_path, exist_ok=True)
os.makedirs(transcription_path, exist_ok=True)
open(videos_db_path, 'a').close()
open(processed_path, 'a').close()

# Function to read CSV and get a list of filenames
def read_csv(csv_path):
    with open(csv_path, newline='') as csvfile:
        reader = csv.reader(csvfile)
        return [row[0] for row in reader]

# Function to check if a file is listed in a CSV, and if not, add it
def update_csv_if_new(file_path, csv_path):
    existing_files = read_csv(csv_path)
    if file_path not in existing_files:
        with open(csv_path, 'a', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow([file_path])

# Function to process the file, generate zip, and send to lazyingart server
def process_and_publish_file(
    file_path, 
    publish_xhs, publish_bilibili, publish_douyin, publish_shipinhao, publish_y2b, 
    test_mode, 
    use_cache,
    use_translation_cache=False,
    use_metadata_cache=False
):
    upload_url = 'http://localhost:8081/upload'
    process_url = 'http://localhost:8081/video-processing'
    
    # Create an instance of VideoProcessor and process the video
    print("Processing file...")
    processor = VideoProcessor(upload_url, process_url, file_path, transcription_path)
    zip_file_path = processor.process_video(
        use_cache=use_cache,
        use_translation_cache=use_translation_cache,
        use_metadata_cache=use_metadata_cache
    )

    if zip_file_path:
        # Send zip file to lazyingart server for publishing
        publish_url = 'http://lazyingart:8081/publish'
        with open(zip_file_path, 'rb') as f:
            files = {'file': (os.path.basename(zip_file_path), f)}
            data = {
                'publish_xhs': str(publish_xhs).lower(),
                'publish_bilibili': str(publish_bilibili).lower(),
                'publish_douyin': str(publish_douyin).lower(),
                'publish_shipinhao': str(publish_shipinhao).lower(),
                'publish_y2b': str(publish_y2b).lower(),
                'test': str(test_mode).lower(),
                'filename': os.path.basename(zip_file_path),
            }
            print(f"Publishing {zip_file_path}")
            response = requests.post(publish_url, files=files, data=data)
            print(f"Response: {response.text}")
    else:
        print(f"Failed to process video: {file_path}")

if __name__ == "__main__":
    # Lock file is now relative to script directory
    lock_file_path = os.path.join(SCRIPT_DIR, "autopub.lock")
    bash_script_path = os.path.join(SCRIPT_DIR, "autopub.sh")

    # Check if lock file exists, if not, create it
    if not os.path.exists(lock_file_path):
        open(lock_file_path, 'a').close()

    # Parse command line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument('--pub-xhs', action='store_true', help="Publish on XiaoHongShu")
    parser.add_argument('--pub-bilibili', action='store_true', help="Publish on Bilibili")
    parser.add_argument('--pub-douyin', action='store_true', help="Publish on DouYin")
    parser.add_argument('--pub-shipinhao', action='store_true', help="Publish on ShiPinHao")
    parser.add_argument('--pub-y2b', action='store_true', help="Publish on YouTube")
    parser.add_argument('--no-pub', action='store_true', help="Don't publish anywhere")
    parser.add_argument('--test', action='store_true', help="Run in test mode")
    parser.add_argument('--use-cache', action='store_true', help="Use cache")
    parser.add_argument('--use-translation-cache', action='store_true', help="Use translation cache")
    parser.add_argument('--use-metadata-cache', action='store_true', help="Use metadata cache")
    parser.add_argument('--force', action='store', type=str, help="Force update the file followed by the --force argument")
    parser.add_argument('--path', action='store', type=str, help="Process only the file at this path")
    args = parser.parse_args()

    # Determine publishing platforms based on provided arguments
    # If none of the publish_xxx flags are provided, default to publishing on all platforms
    if not any([args.pub_xhs, args.pub_bilibili, args.pub_douyin, args.pub_y2b, args.pub_shipinhao]):
        publish_xhs = publish_bilibili = publish_douyin = publish_y2b = publish_shipinhao = True
    else:
        publish_xhs = args.pub_xhs
        publish_bilibili = args.pub_bilibili
        publish_douyin = args.pub_douyin
        publish_shipinhao = args.pub_shipinhao
        publish_y2b = args.pub_y2b

    if args.no_pub:
        publish_xhs = False
        publish_bilibili = False
        publish_douyin = False
        publish_shipinhao = False
        publish_y2b = False

    test_mode = args.test
    use_cache = args.use_cache
    force_filename = args.force
    
    if not (force_filename is None):
        force_filename = force_filename.strip()
    else:
        force_filename = ""
    
    force_files = force_filename.split(",")
    use_translation_cache = args.use_translation_cache
    use_metadata_cache = args.use_metadata_cache

    current_datetime = datetime.now()
    log_filename = f"{current_datetime.strftime('%Y-%m-%d %H-%M-%S')}.txt"
    log_file_path = os.path.join(logs_folder_path, log_filename)

    # Define video file pattern
    video_file_pattern = re.compile(r'.+\.(mp4|mov|avi|flv|wmv|mkv)$', re.IGNORECASE)
    
    # Single file mode
    if args.path:
        filename = os.path.basename(args.path)
        if video_file_pattern.match(filename):
            processed_files = read_csv(processed_path)
            if filename not in processed_files or force_filename:
                print("process and publish file: ", args.path)
                process_and_publish_file(
                    args.path,
                    publish_xhs=publish_xhs,
                    publish_bilibili=publish_bilibili,
                    publish_douyin=publish_douyin,
                    publish_y2b=publish_y2b,
                    publish_shipinhao=publish_shipinhao,
                    test_mode=test_mode,
                    use_cache=use_cache,
                    use_translation_cache=use_translation_cache,
                    use_metadata_cache=use_metadata_cache
                )
                update_csv_if_new(filename, processed_path)
        else:
            print(f"The file {filename} does not match the video file pattern or has already been processed.")
    else:
        # Check each file in the autopublish folder
        for filename in os.listdir(autopublish_folder_path):
            if filename.startswith("preprocessed"):
                update_csv_if_new(filename, processed_path)
                continue

            if video_file_pattern.match(filename):
                file_path = os.path.join(autopublish_folder_path, filename)
                if os.path.isfile(file_path):
                    # Check and update videos_db.csv
                    update_csv_if_new(filename, videos_db_path)
                    
                    processed_files = read_csv(processed_path)
                    # If not processed, process the file and update processed.csv
                    if ((force_files and any(force_file.strip() in filename for force_file in force_files)) or 
                        (filename and filename in force_files)) or (not force_filename and filename not in processed_files):
                        print("process and publish file: ", file_path)
                        process_and_publish_file(
                            file_path,
                            publish_xhs=publish_xhs,
                            publish_bilibili=publish_bilibili,
                            publish_douyin=publish_douyin,
                            publish_y2b=publish_y2b,
                            publish_shipinhao=publish_shipinhao,
                            test_mode=test_mode,
                            use_cache=use_cache,
                            use_translation_cache=use_translation_cache,
                            use_metadata_cache=use_metadata_cache
                        )
                        update_csv_if_new(filename, processed_path)

    # After all tasks are done, remove the lock file
    if os.path.exists(lock_file_path):
        os.remove(lock_file_path)

    # Re-execute the Bash script (commented out in original)
    # print("Re-executing the Bash script...")
    # subprocess.run(["bash", bash_script_path])

===== ./process_queue.sh =====
#!/bin/bash
# process_queue.sh - Processes files from the queue

# Get the directory where this script is located
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Define variables
QUEUE_LIST="${SCRIPT_DIR}/queue_list.txt"
LOG_DIR="${SCRIPT_DIR}/logs-autopub"
PUBLISH_SCRIPT="${SCRIPT_DIR}/autopub.sh"
QUEUE_LOCK="${SCRIPT_DIR}/queue.lock"

# Function to echo with timestamp
echo_with_timestamp() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1"
}

# Ensure the log directory and list file exist
mkdir -p "${LOG_DIR}"
touch "${QUEUE_LIST}"
touch "${QUEUE_LOCK}"
echo_with_timestamp "Starting process_queue.sh script..."

# Main loop to process files from the queue
echo_with_timestamp "Entering main processing loop..."
while true; do
    TIMESTAMP=$(date +%s)
    TMP_FILE="/tmp/queue_path_$TIMESTAMP.txt"
    {
        flock -x 200
        if [ -s "$QUEUE_LIST" ]; then
            full_path=$(head -n 1 "$QUEUE_LIST")
            echo "$full_path" > "$TMP_FILE"
            echo_with_timestamp "Read from queue inside lock: $full_path"
        else
            echo "" > "$TMP_FILE"
        fi
    } 200>"$QUEUE_LOCK"

    full_path=$(cat "$TMP_FILE")
    echo_with_timestamp "Variable full_path after lock: $full_path"
    
    if [ -n "$full_path" ]; then
        echo_with_timestamp "Processing: ${full_path}"
        bash "$PUBLISH_SCRIPT" "${full_path}" &>> "${LOG_DIR}/autopub.log"
        result=$?
        if [ $result -eq 0 ]; then
            echo_with_timestamp "Processing completed for: ${full_path}"
            {
                flock -x 200
                sed -i '1d' "$QUEUE_LIST"
                echo_with_timestamp "Removed from queue: $full_path"
            } 200>"$QUEUE_LOCK"
        else
            echo_with_timestamp "Processing failed for: ${full_path} with error code $result"
        fi
        
        rm "$TMP_FILE"
    else
        # echo_with_timestamp "No valid file to process. Waiting for new files in the queue..."
        sleep 1
    fi
done

===== ./requeue.sh =====


#!/bin/bash
# requeue.sh - Requeue files for processing

# Get the directory where this script is located
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Directory to observe (data path remains unchanged)
OBSERVE_DIR="${HOME}/AutoPublishDATA/AutoPublish"
QUEUE_PIPE="${SCRIPT_DIR}/queue.pipe"

# Confirmation flag
SKIP_CONFIRMATION=false

# Helper function to queue file
queue_file() {
    local file_path=$1
    # Ensure the pipe exists
    if [ ! -p "$QUEUE_PIPE" ]; then
        mkfifo "$QUEUE_PIPE"
    fi
    echo "$file_path" > "$QUEUE_PIPE" &
    echo "Queued: $file_path"
}

# Process flags
while getopts "y" opt; do
    case $opt in
        y) SKIP_CONFIRMATION=true ;;
        \?) echo "Invalid option: -$OPTARG" >&2; exit 1 ;;
    esac
done
shift $((OPTIND -1))

# Check if any IDs are provided
if [ $# -eq 0 ]; then
    echo "Usage: $0 [-y] pattern_or_full_path"
    echo "  -y    Skip confirmation for multiple matches"
    echo "  pattern_or_full_path can be a full file path or a pattern to match files in $OBSERVE_DIR"
    exit 1
fi

# Support full paths or patterns
input="$1"
if [[ "$input" == *"/"* ]]; then
    # If the input contains a slash, treat it as a full path
    if [ -f "$input" ]; then
        queue_file "$input"
    else
        echo "File does not exist: $input"
        exit 1
    fi
else
    # Ensure the observe directory exists
    if [ ! -d "$OBSERVE_DIR" ]; then
        mkdir -p "$OBSERVE_DIR"
        echo "Created observation directory: $OBSERVE_DIR"
    fi

    # Use find to handle patterns and special characters
    mapfile -t matches < <(find "$OBSERVE_DIR" -type f -iname "*$input*")

    # Check for no matches
    if [ ${#matches[@]} -eq 0 ]; then
        echo "No files matched in $OBSERVE_DIR"
        exit 0
    fi

    # Function to handle file selection and queuing
    handle_queueing() {
        if [ "$SKIP_CONFIRMATION" = true ] || [ ${#matches[@]} -eq 1 ]; then
            queue_file "${matches[0]}"
        else
            echo "Multiple files matched. Please select the file(s) to queue (comma-separated numbers):"
            local i=1
            for f in "${matches[@]}"; do
                echo "$i) $f"
                ((i++))
            done
            read -p "#? " selection
            IFS=',' read -r -a selections <<< "$selection"
            for sel in "${selections[@]}"; do
                # Validate selection
                if [[ "$sel" =~ ^[0-9]+$ ]] && [ "$sel" -ge 1 ] && [ "$sel" -le ${#matches[@]} ]; then
                    queue_file "${matches[$sel-1]}"
                else
                    echo "Invalid selection: $sel"
                fi
            done
        fi
    }

    # Queue files based on confirmation requirement
    handle_queueing
fi

# Background any ongoing jobs
disown -a

echo "Queue operation completed successfully."

===== ./autopub.sh =====
#!/bin/bash
# autopub.sh - Script to execute autopub.py with proper environment

# Get the directory where this script is located
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Try to load the user's bash profile to ensure all environment variables are set
source ~/.bashrc 2>/dev/null || source ~/.profile 2>/dev/null || true

# Try to activate Conda environment if available
CONDA_PATH="${HOME}/miniconda3/bin/activate"
CONDA_ENV="autopub-video"

if [ -f "$CONDA_PATH" ]; then
    source "$CONDA_PATH" "$CONDA_ENV" || echo "Warning: Could not activate conda environment $CONDA_ENV"
else
    echo "Warning: Conda not found at $CONDA_PATH, using system Python"
fi

# Capture the first argument as the full path
full_path="$1"

# Function to echo with timestamp
echo_with_timestamp() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1"
}

echo_with_timestamp "Executing autopub.py with file: ${full_path}..."

# Define the lock file and log file
lock_file="${SCRIPT_DIR}/autopub.lock"
log_dir="${SCRIPT_DIR}/logs-autopub"
log_file="${log_dir}/autopub_$(date '+%Y-%m-%d_%H-%M-%S').log"

# Create log directory if it doesn't exist
mkdir -p "${log_dir}"

# Wait for lock file to be released
while [ -f "${lock_file}" ]; do
    echo_with_timestamp "Another instance of the script is running. Waiting..."
    sleep 10  # Adjusted to check every 10 seconds
done

# Create a lock file
touch "${lock_file}"

# Ensure the lock file is removed when the script finishes
trap 'rm -f "${lock_file}"; exit' INT TERM EXIT

echo_with_timestamp "Executing autopub.py..."

# Find python in conda env or use system python
if [ -n "$CONDA_PREFIX" ]; then
    PYTHON_CMD="${CONDA_PREFIX}/bin/python"
else
    PYTHON_CMD="python3"
fi

if [ -n "${full_path}" ]; then
    # If a full path is provided, run the script with the --path argument
    echo_with_timestamp "Processing file: ${full_path}..."
    sleep 10
    $PYTHON_CMD "${SCRIPT_DIR}/autopub.py" --use-cache --use-metadata-cache --use-translation-cache --path "${full_path}" > "${log_file}" 2>&1
else
    sleep 10
    # If no path is provided, run the script without the --path argument
    $PYTHON_CMD "${SCRIPT_DIR}/autopub.py" --use-cache --use-metadata-cache --use-translation-cache > "${log_file}" 2>&1
fi

echo_with_timestamp "Finished executing autopub.py with file: ${full_path}..."

# Remove the lock file and clear the trap
rm -f "${lock_file}"
trap - INT TERM EXIT

echo_with_timestamp "Finished autopub.sh..."

===== ./queue_list.txt =====


===== ./install_autopub_monitor.sh =====
#!/bin/bash
# install-autopub-monitor.sh
# Installation script for autopub-monitor system

set -e

# Text formatting
BOLD="\033[1m"
GREEN="\033[0;32m"
YELLOW="\033[1;33m"
RED="\033[0;31m"
NC="\033[0m" # No Color

# Print with timestamp
log() {
    echo -e "$(date '+%Y-%m-%d %H:%M:%S') - $1"
}

# Print warning
warning() {
    echo -e "${YELLOW}Warning: $1${NC}"
}

# Print error and exit
error() {
    echo -e "${RED}Error: $1${NC}"
    exit 1
}

# Check if script is run as root
check_root() {
    if [ "$EUID" -ne 0 ]; then
        error "This script must be run as root (use sudo)"
    fi
}

# Check package installation
check_package() {
    if ! dpkg -l | grep -q "^ii  $1 "; then
        log "Package $1 is not installed. Installing..."
        apt-get install -y $1 || error "Failed to install $1"
    else
        log "Package $1 is already installed."
    fi
}

# Make sure required files are executable
ensure_executables() {
    local current_dir=$(pwd)
    local scripts=(
        "autopub_monitor_tmux_session.sh"
        "autopub_sync.sh"
        "monitor_autopublish.sh"
        "process_queue.sh"
        "requeue.sh"
    )
    
    for script in "${scripts[@]}"; do
        if [ -f "$current_dir/$script" ]; then
            chmod +x "$current_dir/$script"
            log "Made $script executable"
        else
            warning "Script not found: $script (not critical, but may be needed)"
        fi
    done
    
    # Check for critical script
    if [ ! -f "$current_dir/autopub_monitor_tmux_session.sh" ]; then
        error "Required script not found: autopub_monitor_tmux_session.sh"
    fi
}

# Create named pipe for queue if it doesn't exist
create_named_pipe() {
    local current_dir=$(pwd)
    local pipe_path="$current_dir/queue.pipe"
    
    if [ ! -p "$pipe_path" ]; then
        log "Creating named pipe at $pipe_path"
        mkfifo "$pipe_path"
    else
        log "Named pipe already exists at $pipe_path"
    fi
}

# Create necessary files if they don't exist
create_empty_files() {
    local current_dir=$(pwd)
    local files=(
        "queue_list.txt"
        "temp_queue.txt"
        "checked_list.txt"
        "queue.lock"
        "processed.csv"
        "videos_db.csv"
        "ignore_list.txt"
    )
    
    for file in "${files[@]}"; do
        if [ ! -f "$current_dir/$file" ]; then
            log "Creating empty file: $file"
            touch "$current_dir/$file"
        else
            log "File already exists: $file"
        fi
    done
}

# Install systemd service
install_service() {
    local service_name="autopub-monitor.service"
    local service_path="/etc/systemd/system/$service_name"
    local current_dir=$(pwd)
    local username=$(stat -c '%U' "$current_dir/autopub_monitor_tmux_session.sh")
    
    log "Creating systemd service file: $service_path"
    
    cat > "$service_path" << EOF
[Unit]
Description=AutoPublish Monitoring and Queue Processing
Wants=network-online.target
After=network-online.target

[Service]
User=$username
Type=forking
WorkingDirectory=$current_dir
ExecStart=$current_dir/autopub_monitor_tmux_session.sh start
ExecStop=$current_dir/autopub_monitor_tmux_session.sh stop
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
EOF
    
    log "Reloading systemd daemon"
    systemctl daemon-reload
    
    log "Enabling $service_name"
    systemctl enable "$service_name"
    
    log "Starting $service_name"
    systemctl start "$service_name"
    
    log "Service status:"
    systemctl status "$service_name" || true
}

# Create data directories
create_directories() {
    local username=$(stat -c '%U' "$(pwd)/autopub_monitor_tmux_session.sh")
    local user_home=$(eval echo ~$username)
    
    local dirs=(
        "$user_home/AutoPublishDATA"
        "$user_home/AutoPublishDATA/AutoPublish"
        "$user_home/AutoPublishDATA/transcription_data"
        "$(pwd)/logs"
        "$(pwd)/logs-autopub"
    )
    
    for dir in "${dirs[@]}"; do
        if [ ! -d "$dir" ]; then
            log "Creating directory: $dir"
            mkdir -p "$dir"
            chown $username:$username "$dir"
        else
            log "Directory already exists: $dir"
        fi
    done
}

# Install dependencies
install_dependencies() {
    log "Updating package lists..."
    apt-get update || error "Failed to update package lists"
    
    log "Installing required packages..."
    local packages=(
        "tmux"
        "inotify-tools"
        "ffmpeg"
        "python3"
        "python3-pip"
        "rsync"
    )
    
    for pkg in "${packages[@]}"; do
        check_package "$pkg"
    done
    
    log "Installing Python dependencies..."
    pip3 install requests requests-toolbelt selenium || warning "Failed to install some Python dependencies"
}

# Main function
main() {
    log "Starting installation of autopub-monitor service"
    
    check_root
    
    log "Installing dependencies..."
    install_dependencies
    
    log "Setting up required directories..."
    create_directories
    
    log "Creating required files..."
    create_empty_files
    
    log "Setting up queue pipe..."
    create_named_pipe
    
    log "Setting executable permissions..."
    ensure_executables
    
    log "Installing systemd service..."
    install_service
    
    echo -e "\n${BOLD}${GREEN}Installation complete!${NC}"
    echo -e "The autopub-monitor service is now running from $(pwd)"
    echo -e "To check the status, run: ${BOLD}sudo systemctl status autopub-monitor${NC}"
    echo -e "To view logs, run: ${BOLD}sudo journalctl -u autopub-monitor${NC}"
    echo -e "To manually start/stop/restart: ${BOLD}sudo systemctl [start|stop|restart] autopub-monitor${NC}"
}

# Run the main function
main

===== ./README.md =====
# AutoPubMonitor

An automated system for monitoring, processing, and publishing video content to multiple platforms.

## System Overview

AutoPubMonitor is a comprehensive pipeline for video content processing and multi-platform publishing. The system watches for new video files, processes them through a series of steps including transcription and translation, and publishes the results to configured platforms.

### Key Features

- **Automated file detection**: Watches directories for new video content
- **Processing queue management**: Handles videos in a controlled, sequential manner
- **Video processing**: Checks length, formats, and prepares videos
- **Multi-platform publishing**: Supports XiaoHongShu, Bilibili, Douyin, ShiPinHao, and YouTube
- **Caching system**: Optimizes processing by caching results
- **File synchronization**: Handles file movement between systems

## System Components

### Core Processing
- **video_processor_core.py** (formerly autopub.py): Main processing engine that handles video processing and publishing
- **video_processing_client.py** (formerly process_video.py): Client for video processing operations

### Queue Management
- **queue_manager_service.sh** (formerly process_queue.sh): Service that manages the processing queue
- **queue_file_utility.sh** (formerly requeue.sh): Utility for manually adding files to the queue

### Service Management
- **service_manager.sh** (formerly autopub_monitor_tmux_session.sh): Controls all services via tmux sessions
- **process_video_wrapper.sh** (formerly autopub.sh): Environment setup and processing execution
- **file_sync_service.sh** (formerly autopub_sync.sh): File synchronization between systems
- **file_watcher_service.sh** (formerly monitor_autopublish.sh): Watches for new files and adds them to queue

### Utilities
- **window_info_utility.py** (formerly test_xdo.py): Utility to get active window information

## Installation

### Prerequisites

- Linux environment with bash
- Python 3.6+ with required packages
- FFmpeg for video manipulation
- tmux for service management
- inotify-tools for directory monitoring

### Setup

1. Clone this repository:
   ```bash
   git clone https://github.com/lachlanchen/AutoPubMonitor.git
   cd AutoPubMonitor
   ```

2. Install Python dependencies:
   ```bash
   # Create and activate conda environment
   conda create -n autopub-video python=3.8
   conda activate autopub-video
   pip install requests requests_toolbelt selenium
   ```

3. Configure directory paths in the scripts according to your environment.

## Usage

### Starting Services

```bash
./service_manager.sh start
```

This will start:
- Directory monitoring service
- File synchronization service
- Queue processing service
- Transcription sync service

### Stopping Services

```bash
./service_manager.sh stop
```

### Manual Queue Management

To manually add files to the processing queue:

```bash
# Add by pattern match
./queue_file_utility.sh "pattern_to_match"

# Add by full path
./queue_file_utility.sh "/full/path/to/video.mp4"

# Add with auto-confirmation (no selection prompt)
./queue_file_utility.sh -y "pattern_to_match"
```

### Manual Video Processing

```bash
# Process a specific file
./process_video_wrapper.sh "/path/to/video.mp4"

# Process with specific platforms
python video_processor_core.py --pub-xhs --pub-bilibili --path "/path/to/video.mp4"

# Process with caching enabled
python video_processor_core.py --use-cache --use-translation-cache --path "/path/to/video.mp4"
```

## Configuration

- Adjust directory paths in each script according to your setup
- Modify script behavior through command-line arguments
- Platform targets can be configured in `video_processor_core.py`

## Architecture

1. **File Detection**: `file_watcher_service.sh` watches for new files
2. **Queue**: Files are added to `queue_list.txt`
3. **Processing**: `queue_manager_service.sh` processes files using `process_video_wrapper.sh`
4. **Publishing**: Processed files are sent to configured platforms
5. **Tracking**: Processed files are logged in CSV files

## License

Apache License 2.0 - See LICENSE file for details.

===== ./autopub_monitor_tmux_session.sh =====
#!/bin/bash
# autopub_monitor_tmux_session.sh - Manages tmux sessions for autopub monitoring

# Get the directory where this script is located
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Set home directory
export HOME_DIR=~

# Define data directories
AUTO_PUBLISH_DATA="${HOME_DIR}/AutoPublishDATA"
AUTO_PUBLISH_DIR="${AUTO_PUBLISH_DATA}/AutoPublish"
TRANSCRIPTION_DATA="${AUTO_PUBLISH_DATA}/transcription_data"

# Ensure required directories exist
mkdir -p "${AUTO_PUBLISH_DIR}"
mkdir -p "${TRANSCRIPTION_DATA}"

if [ "$1" = "start" ]; then
    # Create the 'video-sync' session if it doesn't exist
    if tmux has-session -t video-sync 2>/dev/null; then
        echo "Session video-sync already exists."
    else
        tmux new-session -d -s video-sync
        tmux send-keys -t video-sync "cd ${SCRIPT_DIR}" C-m
        tmux send-keys -t video-sync "clear" C-m
        tmux send-keys -t video-sync "bash ${SCRIPT_DIR}/autopub_sync.sh" C-m
    fi

    # Start or ensure the monitor-autopub session is running
    if ! tmux has-session -t monitor-autopub 2>/dev/null; then
        tmux new-session -d -s monitor-autopub -c "${AUTO_PUBLISH_DIR}"
        tmux send-keys -t monitor-autopub "cd ${AUTO_PUBLISH_DIR}" C-m
        tmux send-keys -t monitor-autopub "clear" C-m
        tmux send-keys -t monitor-autopub "${SCRIPT_DIR}/monitor_autopublish.sh" C-m
    fi

    # Start or ensure the process-queue session is running
    if ! tmux has-session -t process-queue 2>/dev/null; then
        tmux new-session -d -s process-queue -c "${SCRIPT_DIR}"
        tmux send-keys -t process-queue "cd ${SCRIPT_DIR}" C-m
        tmux send-keys -t process-queue "clear" C-m
        tmux send-keys -t process-queue "${SCRIPT_DIR}/process_queue.sh" C-m
    fi
    
    # Create the 'transcription-sync' session for syncing transcription_data directory
    if ! tmux has-session -t transcription-sync 2>/dev/null; then
        tmux new-session -d -s transcription-sync
        tmux send-keys -t transcription-sync "while true; do rsync -avh --progress ${TRANSCRIPTION_DATA}/ ${HOME_DIR}/jianguoyun/AutoPublishDATA/transcription_data/; sleep 10; done" C-m
    fi
elif [ "$1" = "stop" ]; then
    # Stop all tmux sessions
    tmux kill-session -t video-sync 2>/dev/null
    tmux kill-session -t monitor-autopub 2>/dev/null
    tmux kill-session -t process-queue 2>/dev/null
    tmux kill-session -t transcription-sync 2>/dev/null
    echo "All autopub-monitor tmux sessions have been stopped."
else
    echo "Usage: $0 {start|stop}"
    exit 1
fi

===== ./autopub_sync.sh =====
#!/bin/bash

src="/home/lachlan/jianguoyun/AutoPublishDATA/AutoPublish/"
dst="/home/lachlan/AutoPublishDATA/AutoPublish/"

while true; do
    # Function to check if the filename contains a date in any recognizable format
    contains_date() {
        if [[ $1 =~ [0-9]{4}-[0-9]{2}-[0-9]{2} ]] || [[ $1 =~ VID_[0-9]{4}[0-9]{2}[0-9]{2}_[0-9]{6} ]] || [[ $1 =~ [0-9]{4}_[0-9]{2}_[0-9]{2}_[0-9]{2}_[0-9]{2}_[0-9]{2} ]]; then
            return 0 # True, contains a date
        else
            return 1 # False, does not contain a date
        fi
    }

    # Check for non-zero file size, then rename files using modification time
    process_file() {
        local src_file=$1
        local file_size=$(stat --format="%s" "$src_file")
        
        if [[ "$file_size" -le 0 ]]; then
            echo "File size of $src_file is 0, waiting for transfer to complete..."
            sleep 5
        else
            local filename=$(basename "$src_file")
            local extension="${filename##*.}"
            local base="${filename%.*}"
            local suffix="_COMPLETED"
            local mod_time=$(stat --format="%y" "$src_file" | cut -d'.' -f1 | tr ' :-' '_')
            
            # Check if "_COMPLETED" suffix is already present
            if [[ $filename != *"$suffix"* ]]; then
                if contains_date "$filename"; then
                    new_filename="${base}${suffix}.${extension}"
                else
                    new_filename="${base}_${mod_time}${suffix}.${extension}"
                fi
                
                local new_path=$(dirname "$src_file")/"$new_filename"
                mv "$src_file" "$new_path"
                echo "Renamed $src_file to $new_path"
            else
                # echo "$filename already has the $suffix suffix."
                :
            fi
        fi
    }

    # Process files ensuring they have a non-zero file size
    find "$src" -type f -size +0c | while read src_file; do
        process_file "$src_file"
    done

    # Perform the rsync operation, including only files with the _COMPLETED suffix
    rsync -rt --progress --delete --whole-file --min-size=1 --include="*_COMPLETED.*" --exclude="*" "$src" "$dst"
    
    # Wait before repeating the operation
    sleep 10
done


===== ./test_xdo.py =====
import subprocess

def get_current_window_name():
    try:
        # Get the ID of the active window
        active_window_id = subprocess.check_output(["xdotool", "getactivewindow"]).decode().strip()

        # Get the name of the active window using its ID
        window_name = subprocess.check_output(["xdotool", "getwindowname", active_window_id]).decode().strip()

        return window_name
    except subprocess.CalledProcessError as e:
        print(f"Error: {e.output.decode()}")
        return None

if __name__ == "__main__":
    window_name = get_current_window_name()
    if window_name:
        print(f"The name of the current active window is: '{window_name}'")
    else:
        print("Failed to retrieve the active window name.")



===== ./monitor_autopublish.sh =====
#!/bin/bash
# monitor_autopublish.sh - Monitors directory for new video files

# Get the directory where this script is located
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Paths and initial setup
PUBLISH_SCRIPT="${SCRIPT_DIR}/autopub.sh"
DIRECTORY_TO_OBSERVE="${HOME}/AutoPublishDATA/AutoPublish"
QUEUE_LIST="${SCRIPT_DIR}/queue_list.txt"
TEMP_QUEUE="${SCRIPT_DIR}/temp_queue.txt"
CHECKED_LIST="${SCRIPT_DIR}/checked_list.txt"
QUEUE_LOCK="${SCRIPT_DIR}/queue.lock"

# Function to echo with timestamp
echo_with_timestamp() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1"
}

echo_with_timestamp "Watching directory: $DIRECTORY_TO_OBSERVE for new files or files moved here."

# Ensure necessary files exist
touch "${QUEUE_LIST}"
touch "${TEMP_QUEUE}"
touch "${CHECKED_LIST}"
touch "${QUEUE_LOCK}"

# Check and queue file function
check_and_queue_file() {
    local full_path=$1

    # Check if file has been checked and found invalid before
    if grep -Fxq "$full_path" "${CHECKED_LIST}"; then
        echo_with_timestamp "File $full_path has been checked and is invalid. Skipping."
        return
    fi

    local file_size=$(stat -c %s "$full_path")

    if [ "$file_size" -eq 0 ] || ! ffprobe -v error -show_entries format=filename -of default=noprint_wrappers=1:nokey=1 "$full_path" > /dev/null; then
        sleep 3 # Wait before moving to TEMP_QUEUE
        handle_potential_conflict_file "$full_path"
    else
        queue_file "$full_path"
    fi
}

# Queue file function
queue_file() {
    local file_path=$1
    local sleep_time=$(( RANDOM % 30 + 1 ))  # Random sleep between 1 and 30 seconds
    sleep $sleep_time
    echo_with_timestamp "File $file_path passed checks after a random sleep of $sleep_time seconds. Adding to queue."

    # Lock the queue list, write the file path, and unlock
    (
        flock -x 200
        echo "$file_path" >> "$QUEUE_LIST"
    ) 200>"$QUEUE_LOCK"
}

# Handle potential conflict file function
handle_potential_conflict_file() {
    local original_file_path=$1
    local directory=$(dirname -- "$original_file_path")
    local base_name=$(basename -- "$original_file_path")
    local name_without_ext="${base_name%.*}"
    local prefix=$(echo "$name_without_ext" | sed -E 's/_[0-9]{4}_[0-9]{2}_[0-9]{2}_[0-9]{2}_[0-9]{2}_[0-9]{2}$//')

    # Search for conflict files, considering variable timestamp and NSConflict marker
    for file in "$directory"/*; do
        if [[ "$file" =~ ${prefix}-NSConflict-.* ]] && ffprobe -v error -show_entries format=filename -of default=noprint_wrappers=1:nokey=1 "$file" > /dev/null; then
            echo_with_timestamp "Valid conflict version found: $file. Original file $original_file_path will be skipped from further processing."
            # Mark original file as checked (invalid)
            echo "$original_file_path" >> "${CHECKED_LIST}"
            return
        fi
    done

    # No valid conflict found, and original file is invalid, move it to TEMP_QUEUE
    echo_with_timestamp "No valid conflict found. Original file $original_file_path is invalid. Moving to TEMP_QUEUE."
    echo "$original_file_path" >> "$TEMP_QUEUE"
}

# Function to monitor temp queue
monitor_temp_queue() {
    while true; do
        if [ ! -s "$TEMP_QUEUE" ]; then
            sleep 5
            continue
        fi

        cp "$TEMP_QUEUE" "${TEMP_QUEUE}_copy"
        > "$TEMP_QUEUE"

        while IFS= read -r line; do
            if [ -f "$line" ]; then
                check_and_queue_file "$line"
            fi
        done < "${TEMP_QUEUE}_copy"
        rm "${TEMP_QUEUE}_copy"
    done
}

# Start background monitoring of temp queue
monitor_temp_queue &

# Use inotifywait to monitor the directory for changes
inotifywait -m -e close_write -e moved_to "$DIRECTORY_TO_OBSERVE" |
while read -r directory events filename; do
    if [[ "$filename" =~ ^\..*\..*\..*$ ]]; then
        echo_with_timestamp "Skipping temporary or system file: $filename"
        continue
    fi

    full_path="${directory}${filename}"
    echo_with_timestamp "Significant change detected: $full_path"
    check_and_queue_file "$full_path"
done

===== ./checked_list.txt =====
/home/lachlan/AutoPublishDATA/AutoPublish/C0025_2024_03_04_05_52_32.MP4


===== ./PROJECT_STRUCTURE.md =====
# AutoPubMonitor System Architecture

This document provides an overview of the AutoPubMonitor system architecture, component interactions, and data flow.

## System Overview

AutoPubMonitor is designed as a pipeline for automated video processing and multi-platform publishing. The system follows these main steps:

1. **File detection** - Monitor directories for new video files
2. **Queuing** - Add detected files to a processing queue
3. **Processing** - Process videos (formatting, transcription, etc.)
4. **Publishing** - Send processed content to multiple platforms
5. **Tracking** - Record processed files to prevent duplication

## Component Interactions

```
┌───────────────────┐     ┌───────────────────┐     ┌───────────────────┐
│                   │     │                   │     │                   │
│  file_sync_       ├────►│  file_watcher_    ├────►│  queue_file_      │
│  service.sh       │     │  service.sh       │     │  utility.sh       │
│                   │     │                   │     │                   │
└───────────────────┘     └───────────────────┘     └────────┬──────────┘
                                                             │
                                                             ▼
┌───────────────────┐     ┌───────────────────┐     ┌───────────────────┐
│                   │     │                   │     │                   │
│  service_         │     │  process_video_   │◄────┤  queue_manager_   │
│  manager.sh       ├────►│  wrapper.sh       │     │  service.sh       │
│                   │     │                   │     │                   │
└───────────────────┘     └────────┬──────────┘     └───────────────────┘
                                   │
                                   ▼
                          ┌───────────────────┐     ┌───────────────────┐
                          │                   │     │                   │
                          │  video_processor_ ├────►│  video_processing_│
                          │  core.py         │     │  client.py        │
                          │                   │     │                   │
                          └───────────────────┘     └───────────────────┘
```

## Component Descriptions

### File Management
- **file_sync_service.sh**: Syncs files between directories, renaming with timestamps and "_COMPLETED" suffix
- **file_watcher_service.sh**: Uses inotifywait to monitor directories for new files

### Queue Management
- **queue_file_utility.sh**: Utility for manually adding files to the processing queue
- **queue_manager_service.sh**: Service that processes files from the queue one by one

### Processing Pipeline
- **process_video_wrapper.sh**: Sets up environment and executes the processing script
- **video_processor_core.py**: Core processing logic for videos
- **video_processing_client.py**: Client for handling video upload and receiving processed results

### Service Control
- **service_manager.sh**: Controls all services via tmux sessions
- **window_info_utility.py**: Utility for getting window information

## Data Flow

1. **File Detection**:
   - `file_sync_service.sh` syncs files from source to destination
   - `file_watcher_service.sh` detects new files in the watched directory

2. **Queuing**:
   - New files are added to `queue_list.txt`
   - `queue_file_utility.sh` allows manual addition to the queue

3. **Processing**:
   - `queue_manager_service.sh` reads from the queue
   - `process_video_wrapper.sh` sets up the environment
   - `video_processor_core.py` processes the video file

4. **Video Processing**:
   - `video_processing_client.py` handles:
     - Video length checking
     - Video augmentation if needed
     - Upload to processing server
     - Download of processed results

5. **Publishing**:
   - Processed files are sent to configured platforms through API calls
   - Results are stored in the CSV database

## File Tracking

The system uses several files to track processing status:
- `queue_list.txt`: Current processing queue
- `processed.csv`: Record of already processed files
- `videos_db.csv`: Database of all video files
- `checked_list.txt`: Files that have been checked but found invalid

## Configuration

Each component has configuration at the top of its file:
- Directory paths
- Lock file locations
- Command-line parameters
- Publishing platform settings

These should be adjusted according to your specific environment and requirements.

## Service Management

The `service_manager.sh` script manages multiple tmux sessions:
- **video-sync**: For file synchronization
- **monitor-autopub**: For directory monitoring
- **process-queue**: For queue processing
- **transcription-sync**: For syncing transcription data

Use `./service_manager.sh start` to start all services and `./service_manager.sh stop` to stop them.


===== ./ignore_list.txt =====


===== ./process_video.py =====
import os
import requests
from urllib.parse import urlparse
from pathlib import Path
from requests_toolbelt import MultipartEncoder

import subprocess

import os
import tempfile


def get_video_length(filename):
    """Returns the length of the video in seconds or None if unable to determine."""
    try:
        cmd = f"ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 \"{filename}\""
        output = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT)
        video_length = float(output)
        return video_length
    except Exception as e:
        print(f"Warning: Failed to get video length for {filename}. Error: {e}")
        return None


# def augment_video(video_path, augmented_length, output_path):
#     """
#     Repeats the video to ensure it reaches at least the specified minimum length.
#     If the video already meets or exceeds the minimum length, no repetition is performed.

#     Args:
#         video_path (str): Path to the input video.
#         augmented_length (int): Minimum desired length of the video in seconds.
#         output_path (str): Path to the output augmented video.
#     """
#     # Get the length of the input video
#     video_length = get_video_length(video_path)  # Assume this function is already defined

#     # # Check if repetition is needed
#     # if video_length >= augmented_length:
#     #     print(f"No repetition needed. Video length ({video_length}s) already meets or exceeds the minimum length ({augmented_length}s).")
#     #     if video_path != output_path:
#     #         # Copy the original video to the output path if they are not the same
#     #         shutil.copy(video_path, output_path)
#     #     return output_path

#     # Calculate how many times the video needs to be repeated
#     repeat_count = int(augmented_length / video_length) + (augmented_length % video_length > 0)

#     # Generate a temporary file listing for ffmpeg
#     concat_file_path = "concat_list.txt"
#     with open(concat_file_path, "w") as file:
#         for _ in range(repeat_count):
#             file.write(f"file '{video_path}'\n")

#     # Use ffmpeg to concatenate the video repeats
#     ffmpeg_command = [
#         "ffmpeg", '-y', "-f", "concat", "-safe", "0", "-i", concat_file_path,
#         "-c", "copy", output_path
#     ]
#     subprocess.run(ffmpeg_command, check=True)

#     # Clean up temporary file
#     os.remove(concat_file_path)

#     return output_path

def augment_video(video_path, augmented_length, output_path):
    """
    Repeats the video to ensure it reaches at least the specified minimum length.
    If the video already meets or exceeds the minimum length, no repetition is performed.

    Args:
        video_path (str): Path to the input video.
        augmented_length (int): Minimum desired length of the video in seconds.
        output_path (str): Path to the output augmented video.
    """
    try:
        # Assume get_video_length is already defined and correctly implemented
        video_length = get_video_length(video_path)  # Implement this function to get the video length

        if video_length >= augmented_length:
            print(f"No augmentation needed. Video length ({video_length}s) already meets or exceeds the minimum length ({augmented_length}s).")
            if video_path != output_path:
                # Copy the original video to the output path if they are not the same
                shutil.copy(video_path, output_path)
            return output_path

        repeat_count = int(augmented_length / video_length) + (augmented_length % video_length > 0)
        print(f"Repeating the video {repeat_count} times to meet the minimum length requirement.")

        # Generate a temporary file listing for ffmpeg
        concat_file_path = "concat_list.txt"
        with open(concat_file_path, "w") as file:
            for _ in range(repeat_count):
                file.write(f"file '{video_path}'\n")

        # Update ffmpeg command to re-encode audio for MP4 compatibility
        ffmpeg_command = [
            "ffmpeg", '-y', "-f", "concat", "-safe", "0", "-i", concat_file_path,
            "-c:v", "copy", "-c:a", "aac", "-b:a", "192k", output_path
        ]
        print(f"Executing FFmpeg command: {' '.join(ffmpeg_command)}")
        subprocess.run(ffmpeg_command, check=True)

        print(f"Video successfully augmented and saved to {output_path}")
    except subprocess.CalledProcessError as e:
        print(f"Error during video augmentation: {e}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
    finally:
        # Clean up temporary file
        if os.path.exists(concat_file_path):
            os.remove(concat_file_path)

    return output_path

class VideoProcessor:



    def __init__(self, upload_url, process_url, video_path, transcription_path):
        self.upload_url = upload_url
        self.process_url = process_url
        self.video_path = video_path
        self.transcription_path = transcription_path
        os.makedirs(self.transcription_path, exist_ok=True)



        input_file = self.video_path
        # Define the minimum length for the video in seconds
        augmented_length = 7  # 7 seconds
        threshold_length = augmented_length

        # Attempt to check the video length
        video_length = get_video_length(input_file)
        
        print("video length: ", video_length)

        # Skip augmentation if the video length is greater than augmented_length or if it couldn't be determined
        if video_length is None or video_length > threshold_length:
        # if False:
            if video_length is None:
                print(f"Warning: Could not determine video length for {input_file}. Skipping augmentation.")
            else:
                print(f"Video is longer than {augmented_length} seconds, skipping augmentation.")
            
        else:
            # Proceed with augmentation if the video is shorter than the augmented_length
            # Ensure augment_video_if_needed is properly defined to handle the logic
            print(f"Video length {video_length} is shorter than {threshold_length}. Augmented to {augmented_length}. ")

            # input_file = yield self.augment_video_if_needed(input_file, augmented_length)
            input_file = self.augment_video_if_needed(input_file, augmented_length)

        self.video_path = input_file

    # @run_on_executor
    def augment_video_if_needed(self, input_file, augmented_length):
        print("input_file: ", input_file)

        # base_name, extension = os.path.splitext(os.path.basename(input_file))
        # output_folder = os.path.dirname(input_file)
        # augmented_video_path = os.path.join(output_folder, f"{base_name}_augmented_{augmented_length}s{extension}")

        base_name, extension = os.path.splitext(os.path.basename(input_file))
        # Using tempfile to create a temporary directory
        temp_dir = tempfile.mkdtemp()
        augmented_video_path = os.path.join(temp_dir, f"{base_name}_augmented_{augmented_length}s{extension}")

        print("augmented_video_length: ", augmented_video_path)

        # Perform the augmentation
        new_path = augment_video(input_file, augmented_length, augmented_video_path)
        return new_path

    def process_video(self, 
        use_cache=False,
        use_translation_cache=False,
        use_metadata_cache=False
    ):



        video_name = Path(self.video_path).stem
        zip_file_root = os.path.join(self.transcription_path, video_name)
        os.makedirs(zip_file_root, exist_ok=True)
        zip_file_path = os.path.join(zip_file_root, f"{video_name}.zip")

        # Check cache
        if use_cache and os.path.isfile(zip_file_path):
            print(f"Cache hit! Returning the processed file from {zip_file_path}.")
            return zip_file_path

        else:
            if not os.path.isfile(zip_file_path):
                print(f"{zip_file_path} not found. ")
                print("Cache miss. Uploading video for processing.")
            else:
                print("Cache ignored: use_cache=false.")
            

        
        if not self.upload_url.endswith("stream"):
            with open(self.video_path, 'rb') as f:
                files = {'video': (os.path.basename(self.video_path), f)}
                response = requests.post(
                    self.upload_url, 
                    files=files, 
                    data={'filename': os.path.basename(self.video_path)}
                )
        else:
            # Preprocess the file for streaming upload
            preprocessed_file_path = self.preprocess_for_streaming(self.video_path)
            with open(preprocessed_file_path, 'rb') as f:
                files = {'video': (os.path.basename(preprocessed_file_path), f)}
                response = requests.put(
                    self.upload_url, 
                    files=files, 
                    params={'filename': os.path.basename(preprocessed_file_path)}
                )
 


        if not response.ok:
            print(f'Failed to upload file. Status code: {response.status_code}, Message: {response.text}')
            return

        # Extract the file path from the response
        uploaded_file_path = response.json().get('file_path')
        if not uploaded_file_path:
            print("Failed to get the uploaded file path from the server response.")
            return

        # Request processing of the uploaded file
        process_response = requests.post(
            self.process_url, 
            data={
                'file_path': uploaded_file_path, 
                "use_translation_cache": use_translation_cache,
                "use_metadata_cache": use_metadata_cache
            }
        )
        if process_response.ok:
            with open(zip_file_path, 'wb') as f:
                f.write(process_response.content)
            print(f'Success! Processed files are downloaded and saved to {zip_file_path}.')
            return zip_file_path
        else:
            print(f'Failed to process file. Status code: {process_response.status_code}, Message: {process_response.text}')
    
    def preprocess_for_streaming(self, file_path):
        output_file_path = os.path.join(os.path.dirname(file_path), 'preprocessed_' + os.path.basename(file_path))
        # Explicitly specify the video and audio codec along with copying the streams and moving the moov atom
        command = f"ffmpeg -y -i \"{file_path}\" -vcodec copy -acodec copy -movflags faststart \"{output_file_path}\""
        try:
            subprocess.run(command, shell=True, check=True)
            print(f"Successfully preprocessed {file_path} to {output_file_path}")
        except subprocess.CalledProcessError as e:
            print(f"Failed to preprocess file with FFmpeg: {e}")
            return file_path  # Return original file path in case of failure
        return output_file_path



if __name__ == "__main__":
    # Usage
    video_path = '/Users/lachlan/Nutstore Files/Vlog/AutoPublish/IMG_5304.MOV'
    server_url = 'http://lachlanserver:8081/video-processing'
    transcription_path = "/Users/lachlan/Nutstore Files/Vlog/transcription_data"

    processor = VideoProcessor(server_url, video_path, transcription_path)
    processor.process_video()


===== ./temp_queue.txt =====


