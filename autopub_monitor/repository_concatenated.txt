===== ./autopub.py =====
#!/usr/bin/env python3
# autopub.py - Video processing and publishing system

import os
import csv
import re
import json
import argparse
import requests
from datetime import datetime
from pathlib import Path
import subprocess
import sys

# Get the directory where this script is located
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

# Import config
sys.path.append(SCRIPT_DIR)
try:
    from config import CONFIG, init_system
except ImportError:
    print("Error: Cannot import config module.")
    print("Make sure config.py is in the same directory as this script.")
    sys.exit(1)

# Re-initialize system to ensure all directories and files exist
init_system()

# Import local modules
try:
    from process_video import VideoProcessor
    from selenium.webdriver.chrome.service import Service
except ImportError as e:
    print(f"Error importing required modules: {e}")
    print("Make sure all required packages are installed.")
    sys.exit(1)

# Get paths from config
logs_folder_path = CONFIG["logs_dir"]
autopublish_folder_path = CONFIG["auto_publish_dir"]
videos_db_path = CONFIG["videos_db_path"]
processed_path = CONFIG["processed_path"]
transcription_path = CONFIG["transcription_data_dir"]

# Function to read CSV and get a list of filenames
def read_csv(csv_path):
    with open(csv_path, newline='') as csvfile:
        reader = csv.reader(csvfile)
        return [row[0] for row in reader]

# Function to check if a file is listed in a CSV, and if not, add it
def update_csv_if_new(file_path, csv_path):
    existing_files = read_csv(csv_path)
    if file_path not in existing_files:
        with open(csv_path, 'a', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow([file_path])

# Function to process the file, generate zip, and send to lazyingart server
def process_and_publish_file(
    file_path, 
    publish_xhs, publish_bilibili, publish_douyin, publish_shipinhao, publish_y2b, 
    test_mode, 
    use_cache,
    use_translation_cache=False,
    use_metadata_cache=False
):
    upload_url = CONFIG["upload_url"]
    process_url = CONFIG["process_url"]
    publish_url = CONFIG["publish_url"]
    
    # Create an instance of VideoProcessor and process the video
    print("Processing file...")
    processor = VideoProcessor(upload_url, process_url, file_path, transcription_path)
    zip_file_path = processor.process_video(
        use_cache=use_cache,
        use_translation_cache=use_translation_cache,
        use_metadata_cache=use_metadata_cache
    )

    if zip_file_path:
        # Send zip file to lazyingart server for publishing
        with open(zip_file_path, 'rb') as f:
            files = {'file': (os.path.basename(zip_file_path), f)}
            data = {
                'publish_xhs': str(publish_xhs).lower(),
                'publish_bilibili': str(publish_bilibili).lower(),
                'publish_douyin': str(publish_douyin).lower(),
                'publish_shipinhao': str(publish_shipinhao).lower(),
                'publish_y2b': str(publish_y2b).lower(),
                'test': str(test_mode).lower(),
                'filename': os.path.basename(zip_file_path),
            }
            print(f"Publishing {zip_file_path}")
            response = requests.post(publish_url, files=files, data=data)
            print(f"Response: {response.text}")
    else:
        print(f"Failed to process video: {file_path}")

if __name__ == "__main__":
    # Lock file is now relative to script directory
    lock_file_path = os.path.join(SCRIPT_DIR, "autopub.lock")
    bash_script_path = CONFIG["autopub_sh_path"]

    # Check if lock file exists, if not, create it
    if not os.path.exists(lock_file_path):
        open(lock_file_path, 'a').close()

    # Parse command line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument('--pub-xhs', action='store_true', help="Publish on XiaoHongShu")
    parser.add_argument('--pub-bilibili', action='store_true', help="Publish on Bilibili")
    parser.add_argument('--pub-douyin', action='store_true', help="Publish on DouYin")
    parser.add_argument('--pub-shipinhao', action='store_true', help="Publish on ShiPinHao")
    parser.add_argument('--pub-y2b', action='store_true', help="Publish on YouTube")
    parser.add_argument('--no-pub', action='store_true', help="Don't publish anywhere")
    parser.add_argument('--test', action='store_true', help="Run in test mode")
    parser.add_argument('--use-cache', action='store_true', help="Use cache")
    parser.add_argument('--use-translation-cache', action='store_true', help="Use translation cache")
    parser.add_argument('--use-metadata-cache', action='store_true', help="Use metadata cache")
    parser.add_argument('--force', action='store', type=str, help="Force update the file followed by the --force argument")
    parser.add_argument('--path', action='store', type=str, help="Process only the file at this path")
    args = parser.parse_args()

    # Determine publishing platforms based on provided arguments
    # If none of the publish_xxx flags are provided, default to values in config
    default_platforms = CONFIG["default_publish_platforms"]
    
    if not any([args.pub_xhs, args.pub_bilibili, args.pub_douyin, args.pub_y2b, args.pub_shipinhao]):
        publish_xhs = default_platforms["publish_xhs"]
        publish_bilibili = default_platforms["publish_bilibili"]
        publish_douyin = default_platforms["publish_douyin"]
        publish_shipinhao = default_platforms["publish_shipinhao"]
        publish_y2b = default_platforms["publish_y2b"]
    else:
        publish_xhs = args.pub_xhs
        publish_bilibili = args.pub_bilibili
        publish_douyin = args.pub_douyin
        publish_shipinhao = args.pub_shipinhao
        publish_y2b = args.pub_y2b

    if args.no_pub:
        publish_xhs = False
        publish_bilibili = False
        publish_douyin = False
        publish_shipinhao = False
        publish_y2b = False

    test_mode = args.test
    use_cache = args.use_cache
    force_filename = args.force
    
    if not (force_filename is None):
        force_filename = force_filename.strip()
    else:
        force_filename = ""
    
    force_files = force_filename.split(",")
    use_translation_cache = args.use_translation_cache
    use_metadata_cache = args.use_metadata_cache

    current_datetime = datetime.now()
    log_filename = f"{current_datetime.strftime('%Y-%m-%d %H-%M-%S')}.txt"
    log_file_path = os.path.join(logs_folder_path, log_filename)

    # Define video file pattern based on extensions in config
    extensions = "|".join(CONFIG["video_file_extensions"])
    video_file_pattern = re.compile(rf'.+\.({extensions})$', re.IGNORECASE)
    
    # Single file mode
    if args.path:
        filename = os.path.basename(args.path)
        if video_file_pattern.match(filename):
            processed_files = read_csv(processed_path)
            if filename not in processed_files or force_filename:
                print("process and publish file: ", args.path)
                process_and_publish_file(
                    args.path,
                    publish_xhs=publish_xhs,
                    publish_bilibili=publish_bilibili,
                    publish_douyin=publish_douyin,
                    publish_y2b=publish_y2b,
                    publish_shipinhao=publish_shipinhao,
                    test_mode=test_mode,
                    use_cache=use_cache,
                    use_translation_cache=use_translation_cache,
                    use_metadata_cache=use_metadata_cache
                )
                update_csv_if_new(filename, processed_path)
        else:
            print(f"The file {filename} does not match the video file pattern or has already been processed.")
    else:
        # Check each file in the autopublish folder
        for filename in os.listdir(autopublish_folder_path):
            if filename.startswith("preprocessed"):
                update_csv_if_new(filename, processed_path)
                continue

            if video_file_pattern.match(filename):
                file_path = os.path.join(autopublish_folder_path, filename)
                if os.path.isfile(file_path):
                    # Check and update videos_db.csv
                    update_csv_if_new(filename, videos_db_path)
                    
                    processed_files = read_csv(processed_path)
                    # If not processed, process the file and update processed.csv
                    if ((force_files and any(force_file.strip() in filename for force_file in force_files)) or 
                        (filename and filename in force_files)) or (not force_filename and filename not in processed_files):
                        print("process and publish file: ", file_path)
                        process_and_publish_file(
                            file_path,
                            publish_xhs=publish_xhs,
                            publish_bilibili=publish_bilibili,
                            publish_douyin=publish_douyin,
                            publish_y2b=publish_y2b,
                            publish_shipinhao=publish_shipinhao,
                            test_mode=test_mode,
                            use_cache=use_cache,
                            use_translation_cache=use_translation_cache,
                            use_metadata_cache=use_metadata_cache
                        )
                        update_csv_if_new(filename, processed_path)

    # After all tasks are done, remove the lock file
    if os.path.exists(lock_file_path):
        os.remove(lock_file_path)

===== ./process_queue.sh =====
#!/bin/bash
# process_queue.sh - Processes files from the queue

# Get the directory where this script is located
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Source configuration file if it exists
CONFIG_FILE="${SCRIPT_DIR}/autopub_config.sh"
if [ -f "$CONFIG_FILE" ]; then
    source "$CONFIG_FILE"
else
    echo "Warning: Configuration file not found at $CONFIG_FILE"
    echo "Running setup_config.sh to generate configuration..."
    bash "${SCRIPT_DIR}/setup_config.sh" --export
    
    if [ -f "$CONFIG_FILE" ]; then
        source "$CONFIG_FILE"
    else
        echo "Error: Failed to generate configuration. Using default values."
    fi
fi

# Define variables from configuration or use defaults
QUEUE_LIST="${AUTOPUB_QUEUE_LIST_PATH:-${SCRIPT_DIR}/queue_list.txt}"
LOG_DIR="${AUTOPUB_LOGS_AUTOPUB_DIR:-${SCRIPT_DIR}/logs-autopub}"
PUBLISH_SCRIPT="${AUTOPUB_AUTOPUB_SH_PATH:-${SCRIPT_DIR}/autopub.sh}"
QUEUE_LOCK="${AUTOPUB_QUEUE_LOCK_PATH:-${SCRIPT_DIR}/queue.lock}"

# Function to echo with timestamp
echo_with_timestamp() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1"
}

# Ensure the log directory and list file exist
mkdir -p "${LOG_DIR}"
touch "${QUEUE_LIST}"
touch "${QUEUE_LOCK}"
echo_with_timestamp "Starting process_queue.sh script..."

# Main loop to process files from the queue
echo_with_timestamp "Entering main processing loop..."
while true; do
    TIMESTAMP=$(date +%s)
    TMP_FILE="/tmp/queue_path_$TIMESTAMP.txt"
    {
        flock -x 200
        if [ -s "$QUEUE_LIST" ]; then
            full_path=$(head -n 1 "$QUEUE_LIST")
            echo "$full_path" > "$TMP_FILE"
            echo_with_timestamp "Read from queue inside lock: $full_path"
        else
            echo "" > "$TMP_FILE"
        fi
    } 200>"$QUEUE_LOCK"

    full_path=$(cat "$TMP_FILE")
    echo_with_timestamp "Variable full_path after lock: $full_path"
    
    if [ -n "$full_path" ]; then
        echo_with_timestamp "Processing: ${full_path}"
        bash "$PUBLISH_SCRIPT" "${full_path}" &>> "${LOG_DIR}/autopub.log"
        result=$?
        if [ $result -eq 0 ]; then
            echo_with_timestamp "Processing completed for: ${full_path}"
            {
                flock -x 200
                sed -i '1d' "$QUEUE_LIST"
                echo_with_timestamp "Removed from queue: $full_path"
            } 200>"$QUEUE_LOCK"
        else
            echo_with_timestamp "Processing failed for: ${full_path} with error code $result"
        fi
        
        rm "$TMP_FILE"
    else
        # Uncomment this line for more verbose logging
        # echo_with_timestamp "No valid file to process. Waiting for new files in the queue..."
        sleep 1
    fi
done

===== ./requeue.sh =====
#!/bin/bash
# requeue.sh - Requeue files for processing

# Get the directory where this script is located
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Source configuration file if it exists
CONFIG_FILE="${SCRIPT_DIR}/autopub_config.sh"
if [ -f "$CONFIG_FILE" ]; then
    source "$CONFIG_FILE"
else
    echo "Warning: Configuration file not found at $CONFIG_FILE"
    echo "Running setup_config.sh to generate configuration..."
    bash "${SCRIPT_DIR}/setup_config.sh" --export
    
    if [ -f "$CONFIG_FILE" ]; then
        source "$CONFIG_FILE"
    else
        echo "Error: Failed to generate configuration. Using default values."
    fi
fi

# Directory to observe and queue pipe from config or use defaults
OBSERVE_DIR="${AUTOPUB_AUTO_PUBLISH_DIR:-${HOME}/AutoPublishDATA/AutoPublish}"
QUEUE_PIPE="${AUTOPUB_QUEUE_PIPE_PATH:-${SCRIPT_DIR}/queue.pipe}"

# Confirmation flag
SKIP_CONFIRMATION=false

# Helper function to queue file
queue_file() {
    local file_path=$1
    # Ensure the pipe exists
    if [ ! -p "$QUEUE_PIPE" ]; then
        mkfifo "$QUEUE_PIPE"
    fi
    echo "$file_path" > "$QUEUE_PIPE" &
    echo "Queued: $file_path"
}

# Process flags
while getopts "y" opt; do
    case $opt in
        y) SKIP_CONFIRMATION=true ;;
        \?) echo "Invalid option: -$OPTARG" >&2; exit 1 ;;
    esac
done
shift $((OPTIND -1))

# Check if any IDs are provided
if [ $# -eq 0 ]; then
    echo "Usage: $0 [-y] pattern_or_full_path"
    echo "  -y    Skip confirmation for multiple matches"
    echo "  pattern_or_full_path can be a full file path or a pattern to match files in $OBSERVE_DIR"
    exit 1
fi

# Support full paths or patterns
input="$1"
if [[ "$input" == *"/"* ]]; then
    # If the input contains a slash, treat it as a full path
    if [ -f "$input" ]; then
        queue_file "$input"
    else
        echo "File does not exist: $input"
        exit 1
    fi
else
    # Ensure the observe directory exists
    if [ ! -d "$OBSERVE_DIR" ]; then
        mkdir -p "$OBSERVE_DIR"
        echo "Created observation directory: $OBSERVE_DIR"
    fi

    # Use find to handle patterns and special characters
    mapfile -t matches < <(find "$OBSERVE_DIR" -type f -iname "*$input*")

    # Check for no matches
    if [ ${#matches[@]} -eq 0 ]; then
        echo "No files matched in $OBSERVE_DIR"
        exit 0
    fi

    # Function to handle file selection and queuing
    handle_queueing() {
        if [ "$SKIP_CONFIRMATION" = true ] || [ ${#matches[@]} -eq 1 ]; then
            queue_file "${matches[0]}"
        else
            echo "Multiple files matched. Please select the file(s) to queue (comma-separated numbers):"
            local i=1
            for f in "${matches[@]}"; do
                echo "$i) $f"
                ((i++))
            done
            read -p "#? " selection
            IFS=',' read -r -a selections <<< "$selection"
            for sel in "${selections[@]}"; do
                # Validate selection
                if [[ "$sel" =~ ^[0-9]+$ ]] && [ "$sel" -ge 1 ] && [ "$sel" -le ${#matches[@]} ]; then
                    queue_file "${matches[$sel-1]}"
                else
                    echo "Invalid selection: $sel"
                fi
            done
        fi
    }

    # Queue files based on confirmation requirement
    handle_queueing
fi

# Background any ongoing jobs
disown -a

echo "Queue operation completed successfully."

===== ./autopub.sh =====
#!/bin/bash
# autopub.sh - Script to execute autopub.py with proper environment

# Get the directory where this script is located
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Source configuration file if it exists
CONFIG_FILE="${SCRIPT_DIR}/autopub_config.sh"
if [ -f "$CONFIG_FILE" ]; then
    source "$CONFIG_FILE"
else
    echo "Warning: Configuration file not found at $CONFIG_FILE"
    echo "Running setup_config.sh to generate configuration..."
    bash "${SCRIPT_DIR}/setup_config.sh" --export
    
    if [ -f "$CONFIG_FILE" ]; then
        source "$CONFIG_FILE"
    else
        echo "Error: Failed to generate configuration. Using default values."
    fi
fi

# Try to load the user's bash profile to ensure all environment variables are set
source ~/.bashrc 2>/dev/null || source ~/.profile 2>/dev/null || true

# Try to activate Conda environment if available
CONDA_PATH="${AUTOPUB_CONDA_PATH:-${HOME}/miniconda3/bin/activate}"
CONDA_ENV="${AUTOPUB_CONDA_ENV:-autopub-video}"

if [ -f "$CONDA_PATH" ]; then
    source "$CONDA_PATH" "$CONDA_ENV" || echo "Warning: Could not activate conda environment $CONDA_ENV"
else
    echo "Warning: Conda not found at $CONDA_PATH, using system Python"
fi

# Capture the first argument as the full path
full_path="$1"

# Function to echo with timestamp
echo_with_timestamp() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1"
}

echo_with_timestamp "Executing autopub.py with file: ${full_path}..."

# Define the lock file and log file
lock_file="${SCRIPT_DIR}/autopub.lock"
log_dir="${AUTOPUB_LOGS_AUTOPUB_DIR:-${SCRIPT_DIR}/logs-autopub}"
log_file="${log_dir}/autopub_$(date '+%Y-%m-%d_%H-%M-%S').log"

# Create log directory if it doesn't exist
mkdir -p "${log_dir}"

# Wait for lock file to be released
while [ -f "${lock_file}" ]; do
    echo_with_timestamp "Another instance of the script is running. Waiting..."
    sleep 10  # Adjusted to check every 10 seconds
done

# Create a lock file
touch "${lock_file}"

# Ensure the lock file is removed when the script finishes
trap 'rm -f "${lock_file}"; exit' INT TERM EXIT

echo_with_timestamp "Executing autopub.py..."

# Find python in conda env or use system python
if [ -n "$CONDA_PREFIX" ]; then
    PYTHON_CMD="${CONDA_PREFIX}/bin/python"
else
    PYTHON_CMD="python3"
fi

if [ -n "${full_path}" ]; then
    # If a full path is provided, run the script with the --path argument
    echo_with_timestamp "Processing file: ${full_path}..."
    sleep 10
    $PYTHON_CMD "${SCRIPT_DIR}/autopub.py" --use-cache --use-metadata-cache --use-translation-cache --path "${full_path}" > "${log_file}" 2>&1
else
    sleep 10
    # If no path is provided, run the script without the --path argument
    $PYTHON_CMD "${SCRIPT_DIR}/autopub.py" --use-cache --use-metadata-cache --use-translation-cache > "${log_file}" 2>&1
fi

echo_with_timestamp "Finished executing autopub.py with file: ${full_path}..."

# Remove the lock file and clear the trap
rm -f "${lock_file}"
trap - INT TERM EXIT

echo_with_timestamp "Finished autopub.sh..."

===== ./setup_config.sh =====
#!/bin/bash
# setup_config.sh - Configuration setup script

# Get the directory where this script is located
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Function to display usage
usage() {
    echo "Usage: $0 [options]"
    echo
    echo "Options:"
    echo "  -h, --help          Display this help message"
    echo "  -c, --create        Create default configuration only"
    echo "  -i, --initialize    Initialize the system (create configs and directories)"
    echo "  -e, --export        Export Python config to Bash config"
    echo
    echo "Without any options, this script will initialize the system."
}

# Function to check if Python is available
check_python() {
    if ! command -v python3 &> /dev/null; then
        echo "Error: Python 3 is required but not found."
        exit 1
    fi
}

# Function to create default configuration
create_config() {
    check_python
    
    echo "Creating default configuration..."
    python3 "${SCRIPT_DIR}/config.py"
    
    if [ $? -eq 0 ]; then
        echo "Configuration created successfully."
    else
        echo "Error creating configuration."
        exit 1
    fi
}

# Function to initialize the system
initialize_system() {
    check_python
    
    echo "Initializing system..."
    python3 "${SCRIPT_DIR}/config.py"
    
    if [ $? -eq 0 ]; then
        echo "System initialized successfully."
    else
        echo "Error initializing system."
        exit 1
    fi
}

# Function to export configuration to bash
export_config() {
    check_python
    
    echo "Exporting configuration to bash..."
    python3 -c "
import sys
sys.path.insert(0, '${SCRIPT_DIR}')
try:
    from config import CONFIG, export_bash_config
    export_bash_config(CONFIG)
    print('Configuration exported successfully.')
except Exception as e:
    print(f'Error exporting configuration: {e}')
    sys.exit(1)
"
}

# Process command line arguments
if [ $# -eq 0 ]; then
    initialize_system
else
    while [ $# -gt 0 ]; do
        case "$1" in
            -h|--help)
                usage
                exit 0
                ;;
            -c|--create)
                create_config
                shift
                ;;
            -i|--initialize)
                initialize_system
                shift
                ;;
            -e|--export)
                export_config
                shift
                ;;
            *)
                echo "Unknown option: $1"
                usage
                exit 1
                ;;
        esac
    done
fi

# Check if bash config exists before sourcing
if [ -f "${SCRIPT_DIR}/autopub_config.sh" ]; then
    echo "Bash configuration file exists at: ${SCRIPT_DIR}/autopub_config.sh"
    echo "You can source it in your scripts with: source ${SCRIPT_DIR}/autopub_config.sh"
else
    echo "Warning: Bash configuration file not found."
    echo "You may need to run this script with --export option."
fi

exit 0

===== ./autopub_monitor_tmux_session.sh =====
#!/bin/bash
# autopub_monitor_tmux_session.sh - Manages tmux sessions for autopub monitoring

# Get the directory where this script is located
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Source configuration file if it exists
CONFIG_FILE="${SCRIPT_DIR}/autopub_config.sh"
if [ -f "$CONFIG_FILE" ]; then
    source "$CONFIG_FILE"
else
    echo "Warning: Configuration file not found at $CONFIG_FILE"
    echo "Running setup_config.sh to generate configuration..."
    bash "${SCRIPT_DIR}/setup_config.sh" --export
    
    if [ -f "$CONFIG_FILE" ]; then
        source "$CONFIG_FILE"
    else
        echo "Error: Failed to generate configuration. Using default values."
    fi
fi

# Set home directory
export HOME_DIR=~

# Define data directories from config or use defaults
AUTO_PUBLISH_DATA="${AUTOPUB_DATA_DIR:-${HOME_DIR}/AutoPublishDATA}"
AUTO_PUBLISH_DIR="${AUTOPUB_AUTO_PUBLISH_DIR:-${AUTO_PUBLISH_DATA}/AutoPublish}"
TRANSCRIPTION_DATA="${AUTOPUB_TRANSCRIPTION_DATA_DIR:-${AUTO_PUBLISH_DATA}/transcription_data}"

# Define script paths from config or use defaults
SYNC_SCRIPT="${AUTOPUB_SYNC_SCRIPT_PATH:-${SCRIPT_DIR}/autopub_sync.sh}"
MONITOR_SCRIPT="${AUTOPUB_MONITOR_SCRIPT_PATH:-${SCRIPT_DIR}/monitor_autopublish.sh}"
PROCESS_QUEUE_SCRIPT="${AUTOPUB_PROCESS_QUEUE_SCRIPT_PATH:-${SCRIPT_DIR}/process_queue.sh}"

# Ensure required directories exist
mkdir -p "${AUTO_PUBLISH_DIR}"
mkdir -p "${TRANSCRIPTION_DATA}"

# Ensure all scripts are executable
chmod +x "${SYNC_SCRIPT}" 2>/dev/null || echo "Warning: Cannot make ${SYNC_SCRIPT} executable"
chmod +x "${MONITOR_SCRIPT}" 2>/dev/null || echo "Warning: Cannot make ${MONITOR_SCRIPT} executable"
chmod +x "${PROCESS_QUEUE_SCRIPT}" 2>/dev/null || echo "Warning: Cannot make ${PROCESS_QUEUE_SCRIPT} executable"

if [ "$1" = "start" ]; then
    echo "Starting AutoPubMonitor services..."
    
    # Create the 'video-sync' session if it doesn't exist
    if tmux has-session -t video-sync 2>/dev/null; then
        echo "Session video-sync already exists."
    else
        echo "Starting video-sync session..."
        tmux new-session -d -s video-sync
        tmux send-keys -t video-sync "cd ${SCRIPT_DIR}" C-m
        tmux send-keys -t video-sync "clear" C-m
        tmux send-keys -t video-sync "bash ${SYNC_SCRIPT}" C-m
    fi

    # Start or ensure the monitor-autopub session is running
    if ! tmux has-session -t monitor-autopub 2>/dev/null; then
        echo "Starting monitor-autopub session..."
        tmux new-session -d -s monitor-autopub -c "${AUTO_PUBLISH_DIR}"
        tmux send-keys -t monitor-autopub "cd ${AUTO_PUBLISH_DIR}" C-m
        tmux send-keys -t monitor-autopub "clear" C-m
        tmux send-keys -t monitor-autopub "${MONITOR_SCRIPT}" C-m
    fi

    # Start or ensure the process-queue session is running
    if ! tmux has-session -t process-queue 2>/dev/null; then
        echo "Starting process-queue session..."
        tmux new-session -d -s process-queue -c "${SCRIPT_DIR}"
        tmux send-keys -t process-queue "cd ${SCRIPT_DIR}" C-m
        tmux send-keys -t process-queue "clear" C-m
        tmux send-keys -t process-queue "${PROCESS_QUEUE_SCRIPT}" C-m
    fi
    
    # Create the 'transcription-sync' session for syncing transcription_data directory
    if ! tmux has-session -t transcription-sync 2>/dev/null; then
        echo "Starting transcription-sync session..."
        tmux new-session -d -s transcription-sync
        tmux send-keys -t transcription-sync "while true; do rsync -avh --progress ${TRANSCRIPTION_DATA}/ ${HOME_DIR}/jianguoyun/AutoPublishDATA/transcription_data/; sleep 10; done" C-m
    fi
    
    echo "All AutoPubMonitor services started successfully."
    echo "Use 'tmux attach -t SESSION_NAME' to view a specific session:"
    echo "  - video-sync: File synchronization service"
    echo "  - monitor-autopub: File monitoring service"
    echo "  - process-queue: Queue processing service"
    echo "  - transcription-sync: Transcription data sync service"
elif [ "$1" = "stop" ]; then
    echo "Stopping AutoPubMonitor services..."
    
    # Stop all tmux sessions
    for session in video-sync monitor-autopub process-queue transcription-sync; do
        if tmux has-session -t $session 2>/dev/null; then
            echo "Stopping $session session..."
            tmux kill-session -t $session
        else
            echo "Session $session is not running."
        fi
    done
    
    echo "All AutoPubMonitor services have been stopped."
elif [ "$1" = "status" ]; then
    echo "AutoPubMonitor Service Status:"
    
    for session in video-sync monitor-autopub process-queue transcription-sync; do
        if tmux has-session -t $session 2>/dev/null; then
            echo "  - $session: RUNNING"
        else
            echo "  - $session: STOPPED"
        fi
    done
else
    echo "Usage: $0 {start|stop|status}"
    echo
    echo "Commands:"
    echo "  start   - Start all AutoPubMonitor services"
    echo "  stop    - Stop all AutoPubMonitor services"
    echo "  status  - Check the status of all services"
    exit 1
fi

===== ./autopub_sync.sh =====
#!/bin/bash
# autopub_sync.sh - Synchronizes video files from source to destination

# Get the directory where this script is located
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Source configuration file if it exists
CONFIG_FILE="${SCRIPT_DIR}/autopub_config.sh"
if [ -f "$CONFIG_FILE" ]; then
    source "$CONFIG_FILE"
else
    echo "Warning: Configuration file not found at $CONFIG_FILE"
    echo "Running setup_config.sh to generate configuration..."
    bash "${SCRIPT_DIR}/setup_config.sh" --export
    
    if [ -f "$CONFIG_FILE" ]; then
        source "$CONFIG_FILE"
    else
        echo "Error: Failed to generate configuration. Using default values."
    fi
fi

# Define source and destination from config or use defaults
src="${AUTOPUB_SYNC_SOURCE:-${HOME}/jianguoyun/AutoPublishDATA/AutoPublish/}"
dst="${AUTOPUB_SYNC_TARGET:-${HOME}/AutoPublishDATA/AutoPublish/}"
sync_interval="${AUTOPUB_SYNC_INTERVAL:-10}"

echo "Starting AutoPublish sync service"
echo "Source directory: $src"
echo "Destination directory: $dst"
echo "Sync interval: $sync_interval seconds"

# Ensure source and destination directories exist
mkdir -p "$src"
mkdir -p "$dst"

while true; do
    # Function to check if the filename contains a date in any recognizable format
    contains_date() {
        if [[ $1 =~ [0-9]{4}-[0-9]{2}-[0-9]{2} ]] || [[ $1 =~ VID_[0-9]{4}[0-9]{2}[0-9]{2}_[0-9]{6} ]] || [[ $1 =~ [0-9]{4}_[0-9]{2}_[0-9]{2}_[0-9]{2}_[0-9]{2}_[0-9]{2} ]]; then
            return 0 # True, contains a date
        else
            return 1 # False, does not contain a date
        fi
    }

    # Check for non-zero file size, then rename files using modification time
    process_file() {
        local src_file=$1
        local file_size=$(stat --format="%s" "$src_file")
        
        if [[ "$file_size" -le 0 ]]; then
            echo "File size of $src_file is 0, waiting for transfer to complete..."
            sleep 5
        else
            local filename=$(basename "$src_file")
            local extension="${filename##*.}"
            local base="${filename%.*}"
            local suffix="_COMPLETED"
            local mod_time=$(stat --format="%y" "$src_file" | cut -d'.' -f1 | tr ' :-' '_')
            
            # Check if "_COMPLETED" suffix is already present
            if [[ $filename != *"$suffix"* ]]; then
                if contains_date "$filename"; then
                    new_filename="${base}${suffix}.${extension}"
                else
                    new_filename="${base}_${mod_time}${suffix}.${extension}"
                fi
                
                local new_path=$(dirname "$src_file")/"$new_filename"
                mv "$src_file" "$new_path"
                echo "Renamed $src_file to $new_path"
            else
                # echo "$filename already has the $suffix suffix."
                :
            fi
        fi
    }

    echo "Checking for files to process at $(date)"
    
    # Process files ensuring they have a non-zero file size
    if [ -d "$src" ]; then
        find "$src" -type f -size +0c | while read src_file; do
            process_file "$src_file"
        done
    fi

    echo "Starting rsync at $(date)"
    
    # Perform the rsync operation, including only files with the _COMPLETED suffix
    rsync -rt --progress --delete --whole-file --min-size=1 --include="*_COMPLETED.*" --exclude="*" "$src" "$dst"
    
    echo "Finished rsync at $(date), waiting $sync_interval seconds"
    
    # Wait before repeating the operation
    sleep $sync_interval
done

===== ./config.py =====
#!/usr/bin/env python3
# config.py - Central configuration for AutoPubMonitor system

import os
import json
from pathlib import Path

# Base directories
HOME_DIR = os.path.expanduser("~")
DEFAULT_BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# Data directories
DEFAULT_DATA_DIR = os.path.join(HOME_DIR, "AutoPublishDATA")
DEFAULT_AUTO_PUBLISH_DIR = os.path.join(DEFAULT_DATA_DIR, "AutoPublish")
DEFAULT_TRANSCRIPTION_DATA_DIR = os.path.join(DEFAULT_DATA_DIR, "transcription_data")

# Configuration file path
CONFIG_FILE = os.path.join(DEFAULT_BASE_DIR, "autopub_config.json")

# Default configuration
DEFAULT_CONFIG = {
    # System paths
    "base_dir": DEFAULT_BASE_DIR,
    "data_dir": DEFAULT_DATA_DIR,
    "auto_publish_dir": DEFAULT_AUTO_PUBLISH_DIR,
    "transcription_data_dir": DEFAULT_TRANSCRIPTION_DATA_DIR,
    "logs_dir": os.path.join(DEFAULT_BASE_DIR, "logs"),
    "logs_autopub_dir": os.path.join(DEFAULT_BASE_DIR, "logs-autopub"),

    # Database files
    "videos_db_path": os.path.join(DEFAULT_BASE_DIR, "videos_db.csv"),
    "processed_path": os.path.join(DEFAULT_BASE_DIR, "processed.csv"),
    
    # Queue files
    "queue_list_path": os.path.join(DEFAULT_BASE_DIR, "queue_list.txt"),
    "temp_queue_path": os.path.join(DEFAULT_BASE_DIR, "temp_queue.txt"),
    "checked_list_path": os.path.join(DEFAULT_BASE_DIR, "checked_list.txt"),
    "queue_lock_path": os.path.join(DEFAULT_BASE_DIR, "queue.lock"),
    "queue_pipe_path": os.path.join(DEFAULT_BASE_DIR, "queue.pipe"),
    
    # Script paths
    "autopub_script_path": os.path.join(DEFAULT_BASE_DIR, "autopub.py"),
    "autopub_sh_path": os.path.join(DEFAULT_BASE_DIR, "autopub.sh"),
    "monitor_script_path": os.path.join(DEFAULT_BASE_DIR, "monitor_autopublish.sh"),
    "process_queue_script_path": os.path.join(DEFAULT_BASE_DIR, "process_queue.sh"),
    "sync_script_path": os.path.join(DEFAULT_BASE_DIR, "autopub_sync.sh"),
    
    # Server URLs
    "upload_url": "http://localhost:8081/upload",
    "process_url": "http://localhost:8081/video-processing",
    "publish_url": "http://lazyingart:8081/publish",
    
    # Publishing platforms default (when no flags provided)
    "default_publish_platforms": {
        "publish_xhs": True,
        "publish_bilibili": True,
        "publish_douyin": True, 
        "publish_shipinhao": True,
        "publish_y2b": True
    },
    
    # Sync configuration
    "sync_source": os.path.join(HOME_DIR, "jianguoyun", "AutoPublishDATA", "AutoPublish"),
    "sync_target": os.path.join(HOME_DIR, "AutoPublishDATA", "AutoPublish"),
    "sync_interval": 10,  # seconds
    
    # Video processing
    "min_video_length": 7,  # seconds
    "video_file_extensions": ["mp4", "mov", "avi", "flv", "wmv", "mkv"],
    
    # Conda environment
    "conda_path": os.path.join(HOME_DIR, "miniconda3", "bin", "activate"),
    "conda_env": "autopub-video"
}

# Load configuration
def load_config():
    """Load configuration from file, creating default if it doesn't exist"""
    if os.path.isfile(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, 'r') as f:
                config = json.load(f)
            
            # Ensure all default config keys exist in the loaded config
            for key, value in DEFAULT_CONFIG.items():
                if key not in config:
                    config[key] = value
            
            return config
        except Exception as e:
            print(f"Error loading config file: {e}")
            print("Using default configuration instead.")
            return DEFAULT_CONFIG
    else:
        # Create default config file
        save_config(DEFAULT_CONFIG)
        return DEFAULT_CONFIG

# Save configuration
def save_config(config):
    """Save configuration to file"""
    try:
        os.makedirs(os.path.dirname(CONFIG_FILE), exist_ok=True)
        with open(CONFIG_FILE, 'w') as f:
            json.dump(config, f, indent=4)
    except Exception as e:
        print(f"Error saving config file: {e}")

# Create required directories
def create_required_directories(config):
    """Create all required directories from configuration"""
    directories = [
        config["data_dir"],
        config["auto_publish_dir"],
        config["transcription_data_dir"],
        config["logs_dir"],
        config["logs_autopub_dir"]
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)

# Create required files
def create_required_files(config):
    """Create empty files if they don't exist"""
    files = [
        config["videos_db_path"],
        config["processed_path"],
        config["queue_list_path"],
        config["temp_queue_path"],
        config["checked_list_path"],
        config["queue_lock_path"]
    ]
    
    for file_path in files:
        if not os.path.exists(file_path):
            open(file_path, 'a').close()

# Create named pipe
def create_named_pipe(config):
    """Create named pipe if it doesn't exist"""
    pipe_path = config["queue_pipe_path"]
    if not os.path.exists(pipe_path):
        try:
            os.mkfifo(pipe_path)
        except Exception as e:
            print(f"Error creating named pipe: {e}")

# Export config to bash format for shell scripts
def export_bash_config(config, output_path=None):
    """Export configuration to bash format for shell scripts"""
    if output_path is None:
        output_path = os.path.join(os.path.dirname(CONFIG_FILE), "autopub_config.sh")
    
    lines = ["#!/bin/bash", "# AutoPubMonitor configuration", ""]
    
    for key, value in config.items():
        # Handle nested dictionaries
        if isinstance(value, dict):
            for sub_key, sub_value in value.items():
                lines.append(f'export AUTOPUB_{key.upper()}_{sub_key.upper()}="{str(sub_value)}"')
        # Handle strings, numbers, and booleans
        else:
            lines.append(f'export AUTOPUB_{key.upper()}="{str(value)}"')
    
    with open(output_path, 'w') as f:
        f.write('\n'.join(lines))
    
    # Make the file executable
    os.chmod(output_path, 0o755)
    
    return output_path

# Initialize the system
def init_system():
    """Initialize the system with configuration"""
    config = load_config()
    create_required_directories(config)
    create_required_files(config)
    create_named_pipe(config)
    export_bash_config(config)
    return config

# Get configuration
CONFIG = load_config()

if __name__ == "__main__":
    # If run directly, initialize the system
    config = init_system()
    print("System initialized with configuration:")
    for key, value in config.items():
        if not isinstance(value, dict):
            print(f"  {key}: {value}")
        else:
            print(f"  {key}:")
            for sub_key, sub_value in value.items():
                print(f"    {sub_key}: {sub_value}")

===== ./monitor_autopublish.sh =====
#!/bin/bash
# monitor_autopublish.sh - Monitors directory for new video files

# Get the directory where this script is located
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Source configuration file if it exists
CONFIG_FILE="${SCRIPT_DIR}/autopub_config.sh"
if [ -f "$CONFIG_FILE" ]; then
    source "$CONFIG_FILE"
else
    echo "Warning: Configuration file not found at $CONFIG_FILE"
    echo "Running setup_config.sh to generate configuration..."
    bash "${SCRIPT_DIR}/setup_config.sh" --export
    
    if [ -f "$CONFIG_FILE" ]; then
        source "$CONFIG_FILE"
    else
        echo "Error: Failed to generate configuration. Using default values."
    fi
fi

# Paths and initial setup from configuration
PUBLISH_SCRIPT="${AUTOPUB_AUTOPUB_SH_PATH:-${SCRIPT_DIR}/autopub.sh}"
DIRECTORY_TO_OBSERVE="${AUTOPUB_AUTO_PUBLISH_DIR:-${HOME}/AutoPublishDATA/AutoPublish}"
QUEUE_LIST="${AUTOPUB_QUEUE_LIST_PATH:-${SCRIPT_DIR}/queue_list.txt}"
TEMP_QUEUE="${AUTOPUB_TEMP_QUEUE_PATH:-${SCRIPT_DIR}/temp_queue.txt}"
CHECKED_LIST="${AUTOPUB_CHECKED_LIST_PATH:-${SCRIPT_DIR}/checked_list.txt}"
QUEUE_LOCK="${AUTOPUB_QUEUE_LOCK_PATH:-${SCRIPT_DIR}/queue.lock}"

# Function to echo with timestamp
echo_with_timestamp() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1"
}

echo_with_timestamp "Watching directory: $DIRECTORY_TO_OBSERVE for new files or files moved here."

# Ensure necessary files exist
touch "${QUEUE_LIST}"
touch "${TEMP_QUEUE}"
touch "${CHECKED_LIST}"
touch "${QUEUE_LOCK}"

# Check and queue file function
check_and_queue_file() {
    local full_path=$1

    # Check if file has been checked and found invalid before
    if grep -Fxq "$full_path" "${CHECKED_LIST}"; then
        echo_with_timestamp "File $full_path has been checked and is invalid. Skipping."
        return
    fi

    local file_size=$(stat -c %s "$full_path")

    if [ "$file_size" -eq 0 ] || ! ffprobe -v error -show_entries format=filename -of default=noprint_wrappers=1:nokey=1 "$full_path" > /dev/null; then
        sleep 3 # Wait before moving to TEMP_QUEUE
        handle_potential_conflict_file "$full_path"
    else
        queue_file "$full_path"
    fi
}

# Queue file function
queue_file() {
    local file_path=$1
    local sleep_time=$(( RANDOM % 30 + 1 ))  # Random sleep between 1 and 30 seconds
    sleep $sleep_time
    echo_with_timestamp "File $file_path passed checks after a random sleep of $sleep_time seconds. Adding to queue."

    # Lock the queue list, write the file path, and unlock
    (
        flock -x 200
        echo "$file_path" >> "$QUEUE_LIST"
    ) 200>"$QUEUE_LOCK"
}

# Handle potential conflict file function
handle_potential_conflict_file() {
    local original_file_path=$1
    local directory=$(dirname -- "$original_file_path")
    local base_name=$(basename -- "$original_file_path")
    local name_without_ext="${base_name%.*}"
    local prefix=$(echo "$name_without_ext" | sed -E 's/_[0-9]{4}_[0-9]{2}_[0-9]{2}_[0-9]{2}_[0-9]{2}_[0-9]{2}$//')

    # Search for conflict files, considering variable timestamp and NSConflict marker
    for file in "$directory"/*; do
        if [[ "$file" =~ ${prefix}-NSConflict-.* ]] && ffprobe -v error -show_entries format=filename -of default=noprint_wrappers=1:nokey=1 "$file" > /dev/null; then
            echo_with_timestamp "Valid conflict version found: $file. Original file $original_file_path will be skipped from further processing."
            # Mark original file as checked (invalid)
            echo "$original_file_path" >> "${CHECKED_LIST}"
            return
        fi
    done

    # No valid conflict found, and original file is invalid, move it to TEMP_QUEUE
    echo_with_timestamp "No valid conflict found. Original file $original_file_path is invalid. Moving to TEMP_QUEUE."
    echo "$original_file_path" >> "$TEMP_QUEUE"
}

# Function to monitor temp queue
monitor_temp_queue() {
    while true; do
        if [ ! -s "$TEMP_QUEUE" ]; then
            sleep 5
            continue
        fi

        cp "$TEMP_QUEUE" "${TEMP_QUEUE}_copy"
        > "$TEMP_QUEUE"

        while IFS= read -r line; do
            if [ -f "$line" ]; then
                check_and_queue_file "$line"
            fi
        done < "${TEMP_QUEUE}_copy"
        rm "${TEMP_QUEUE}_copy"
    done
}

# Ensure the directory to observe exists
if [ ! -d "$DIRECTORY_TO_OBSERVE" ]; then
    echo_with_timestamp "Creating directory to observe: $DIRECTORY_TO_OBSERVE"
    mkdir -p "$DIRECTORY_TO_OBSERVE"
fi

# Start background monitoring of temp queue
monitor_temp_queue &

# Use inotifywait to monitor the directory for changes
echo_with_timestamp "Starting inotifywait monitoring on: $DIRECTORY_TO_OBSERVE"

# Check if inotifywait is installed
if ! command -v inotifywait &> /dev/null; then
    echo_with_timestamp "Error: inotifywait not found. Please install inotify-tools package."
    exit 1
fi

inotifywait -m -e close_write -e moved_to "$DIRECTORY_TO_OBSERVE" |
while read -r directory events filename; do
    if [[ "$filename" =~ ^\..*\..*\..*$ ]]; then
        echo_with_timestamp "Skipping temporary or system file: $filename"
        continue
    fi

    full_path="${directory}${filename}"
    echo_with_timestamp "Significant change detected: $full_path"
    check_and_queue_file "$full_path"
done

===== ./process_video.py =====
#!/usr/bin/env python3
# process_video.py - Video processing client

import os
import requests
import tempfile
import subprocess
import sys
import shutil
from pathlib import Path
from requests_toolbelt import MultipartEncoder

# Get the directory where this script is located
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

# Import config
sys.path.append(SCRIPT_DIR)
try:
    from config import CONFIG
except ImportError:
    print("Error: Cannot import config module.")
    print("Make sure config.py is in the same directory as this script.")
    sys.exit(1)

def get_video_length(filename):
    """Returns the length of the video in seconds or None if unable to determine."""
    try:
        cmd = f"ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 \"{filename}\""
        output = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT)
        video_length = float(output)
        return video_length
    except Exception as e:
        print(f"Warning: Failed to get video length for {filename}. Error: {e}")
        return None

def augment_video(video_path, augmented_length, output_path):
    """
    Repeats the video to ensure it reaches at least the specified minimum length.
    If the video already meets or exceeds the minimum length, no repetition is performed.

    Args:
        video_path (str): Path to the input video.
        augmented_length (int): Minimum desired length of the video in seconds.
        output_path (str): Path to the output augmented video.
    """
    try:
        video_length = get_video_length(video_path)

        if video_length >= augmented_length:
            print(f"No augmentation needed. Video length ({video_length}s) already meets or exceeds the minimum length ({augmented_length}s).")
            if video_path != output_path:
                # Copy the original video to the output path if they are not the same
                shutil.copy(video_path, output_path)
            return output_path

        repeat_count = int(augmented_length / video_length) + (augmented_length % video_length > 0)
        print(f"Repeating the video {repeat_count} times to meet the minimum length requirement.")

        # Generate a temporary file listing for ffmpeg
        concat_file_path = "concat_list.txt"
        with open(concat_file_path, "w") as file:
            for _ in range(repeat_count):
                file.write(f"file '{video_path}'\n")

        # Update ffmpeg command to re-encode audio for MP4 compatibility
        ffmpeg_command = [
            "ffmpeg", '-y', "-f", "concat", "-safe", "0", "-i", concat_file_path,
            "-c:v", "copy", "-c:a", "aac", "-b:a", "192k", output_path
        ]
        print(f"Executing FFmpeg command: {' '.join(ffmpeg_command)}")
        subprocess.run(ffmpeg_command, check=True)

        print(f"Video successfully augmented and saved to {output_path}")
    except subprocess.CalledProcessError as e:
        print(f"Error during video augmentation: {e}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
    finally:
        # Clean up temporary file
        if os.path.exists(concat_file_path):
            os.remove(concat_file_path)

    return output_path

class VideoProcessor:
    def __init__(self, upload_url, process_url, video_path, transcription_path):
        self.upload_url = upload_url
        self.process_url = process_url
        self.video_path = video_path
        self.transcription_path = transcription_path
        os.makedirs(self.transcription_path, exist_ok=True)

        input_file = self.video_path
        # Get minimum video length from config
        augmented_length = CONFIG["min_video_length"]
        threshold_length = augmented_length

        # Attempt to check the video length
        video_length = get_video_length(input_file)
        
        print("video length: ", video_length)

        # Skip augmentation if the video length is greater than augmented_length or if it couldn't be determined
        if video_length is None or video_length > threshold_length:
            if video_length is None:
                print(f"Warning: Could not determine video length for {input_file}. Skipping augmentation.")
            else:
                print(f"Video is longer than {augmented_length} seconds, skipping augmentation.")
        else:
            # Proceed with augmentation if the video is shorter than the augmented_length
            print(f"Video length {video_length} is shorter than {threshold_length}. Augmented to {augmented_length}. ")
            input_file = self.augment_video_if_needed(input_file, augmented_length)

        self.video_path = input_file

    def augment_video_if_needed(self, input_file, augmented_length):
        print("input_file: ", input_file)

        base_name, extension = os.path.splitext(os.path.basename(input_file))
        # Using tempfile to create a temporary directory
        temp_dir = tempfile.mkdtemp()
        augmented_video_path = os.path.join(temp_dir, f"{base_name}_augmented_{augmented_length}s{extension}")

        print("augmented_video_path: ", augmented_video_path)

        # Perform the augmentation
        new_path = augment_video(input_file, augmented_length, augmented_video_path)
        return new_path

    def process_video(self, 
        use_cache=False,
        use_translation_cache=False,
        use_metadata_cache=False
    ):
        video_name = Path(self.video_path).stem
        zip_file_root = os.path.join(self.transcription_path, video_name)
        os.makedirs(zip_file_root, exist_ok=True)
        zip_file_path = os.path.join(zip_file_root, f"{video_name}.zip")

        # Check cache
        if use_cache and os.path.isfile(zip_file_path):
            print(f"Cache hit! Returning the processed file from {zip_file_path}.")
            return zip_file_path
        else:
            if not os.path.isfile(zip_file_path):
                print(f"{zip_file_path} not found. ")
                print("Cache miss. Uploading video for processing.")
            else:
                print("Cache ignored: use_cache=false.")
            
        if not self.upload_url.endswith("stream"):
            with open(self.video_path, 'rb') as f:
                files = {'video': (os.path.basename(self.video_path), f)}
                response = requests.post(
                    self.upload_url, 
                    files=files, 
                    data={'filename': os.path.basename(self.video_path)}
                )
        else:
            # Preprocess the file for streaming upload
            preprocessed_file_path = self.preprocess_for_streaming(self.video_path)
            with open(preprocessed_file_path, 'rb') as f:
                files = {'video': (os.path.basename(preprocessed_file_path), f)}
                response = requests.put(
                    self.upload_url, 
                    files=files, 
                    params={'filename': os.path.basename(preprocessed_file_path)}
                )

        if not response.ok:
            print(f'Failed to upload file. Status code: {response.status_code}, Message: {response.text}')
            return

        # Extract the file path from the response
        uploaded_file_path = response.json().get('file_path')
        if not uploaded_file_path:
            print("Failed to get the uploaded file path from the server response.")
            return

        # Request processing of the uploaded file
        process_response = requests.post(
            self.process_url, 
            data={
                'file_path': uploaded_file_path, 
                "use_translation_cache": use_translation_cache,
                "use_metadata_cache": use_metadata_cache
            }
        )
        if process_response.ok:
            with open(zip_file_path, 'wb') as f:
                f.write(process_response.content)
            print(f'Success! Processed files are downloaded and saved to {zip_file_path}.')
            return zip_file_path
        else:
            print(f'Failed to process file. Status code: {process_response.status_code}, Message: {process_response.text}')
    
    def preprocess_for_streaming(self, file_path):
        output_file_path = os.path.join(os.path.dirname(file_path), 'preprocessed_' + os.path.basename(file_path))
        # Explicitly specify the video and audio codec along with copying the streams and moving the moov atom
        command = f"ffmpeg -y -i \"{file_path}\" -vcodec copy -acodec copy -movflags faststart \"{output_file_path}\""
        try:
            subprocess.run(command, shell=True, check=True)
            print(f"Successfully preprocessed {file_path} to {output_file_path}")
        except subprocess.CalledProcessError as e:
            print(f"Failed to preprocess file with FFmpeg: {e}")
            return file_path  # Return original file path in case of failure
        return output_file_path


if __name__ == "__main__":
    # Usage example
    video_path = os.path.join(os.path.expanduser("~"), 'AutoPublishDATA', 'AutoPublish', 'example.mp4')
    upload_url = CONFIG["upload_url"]
    process_url = CONFIG["process_url"]
    transcription_path = CONFIG["transcription_data_dir"]

    processor = VideoProcessor(upload_url, process_url, video_path, transcription_path)
    processor.process_video(use_cache=True)

