===== ./autopub.py =====
import os
import csv
import re
import json
import argparse
import requests
from datetime import datetime
from pathlib import Path
from process_video import VideoProcessor
from selenium.webdriver.chrome.service import Service

import subprocess

# Paths for the folders and files
logs_folder_path = '/home/lachlan/ProjectsLFS/autopub_monitor/logs'
autopublish_folder_path = '/home/lachlan/AutoPublishDATA/AutoPublish'
videos_db_path = '/home/lachlan/ProjectsLFS/autopub_monitor/videos_db.csv'
processed_path = '/home/lachlan/ProjectsLFS/autopub_monitor/processed.csv'
transcription_path = "/home/lachlan/AutoPublishDATA/transcription_data"

# Ensure the logs, videos, and database files exist
os.makedirs(logs_folder_path, exist_ok=True)
os.makedirs(autopublish_folder_path, exist_ok=True)
os.makedirs(transcription_path, exist_ok=True)
open(videos_db_path, 'a').close()
open(processed_path, 'a').close()

# Function to read CSV and get a list of filenames
def read_csv(csv_path):
    with open(csv_path, newline='') as csvfile:
        reader = csv.reader(csvfile)
        return [row[0] for row in reader]

# Function to check if a file is listed in a CSV, and if not, add it
def update_csv_if_new(file_path, csv_path):
    existing_files = read_csv(csv_path)
    if file_path not in existing_files:
        with open(csv_path, 'a', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow([file_path])

# Function to process the file, generate zip, and send to lazyingart server
def process_and_publish_file(
    file_path, 
    publish_xhs, publish_bilibili, publish_douyin, publish_shipinhao, publish_y2b, 
    test_mode, 
    use_cache,
    use_translation_cache=False,
    use_metadata_cache=False
):
    upload_url = 'http://localhost:8081/upload'
    process_url = 'http://localhost:8081/video-processing'
    
    # Create an instance of VideoProcessor and process the video
    print("Processing file...")
    processor = VideoProcessor(upload_url, process_url, file_path, transcription_path)
    zip_file_path = processor.process_video(
        use_cache=use_cache,
        use_translation_cache=use_translation_cache,
        use_metadata_cache=use_metadata_cache
    )

    if zip_file_path:
        # Send zip file to lazyingart server for publishing
        publish_url = 'http://lazyingart:8081/publish'
        with open(zip_file_path, 'rb') as f:
            files = {'file': (os.path.basename(zip_file_path), f)}
            data = {
                'publish_xhs': str(publish_xhs).lower(),
                'publish_bilibili': str(publish_bilibili).lower(),
                'publish_douyin': str(publish_douyin).lower(),
                'publish_shipinhao': str(publish_shipinhao).lower(),
                'publish_y2b': str(publish_y2b).lower(),
                'test': str(test_mode).lower(),
                'filename': os.path.basename(zip_file_path),
            }
            print(f"Publishing {zip_file_path}")
            response = requests.post(publish_url, files=files, data=data)
            print(f"Response: {response.text}")
    else:
        print(f"Failed to process video: {file_path}")

if __name__ == "__main__":

    lock_file_path = "/home/lachlan/Projects/autopub_monitor/autopub.lock"
    bash_script_path = "/home/lachlan/Projects/autopub_monitor/autopub.sh"

    # Check if lock file exists, if not, create it
    if not os.path.exists(lock_file_path):
        open(lock_file_path, 'a').close()


    # Parse command line arguments
    parser = argparse.ArgumentParser()
    parser.add_argument('--pub-xhs', action='store_true', help="Publish on XiaoHongShu")
    parser.add_argument('--pub-bilibili', action='store_true', help="Publish on Bilibili")
    parser.add_argument('--pub-douyin', action='store_true', help="Publish on DouYin")
    parser.add_argument('--pub-shipinhao', action='store_true', help="Publish on ShiPinHao")
    parser.add_argument('--pub-y2b', action='store_true', help="Publish on YouTube")
    parser.add_argument('--no-pub', action='store_true', help="Publish on YouTube")
    parser.add_argument('--test', action='store_true', help="Run in test mode")
    parser.add_argument('--use-cache', action='store_true', help="Use cache")
    parser.add_argument('--use-translation-cache', action='store_true', help="Use translation cache")
    parser.add_argument('--use-metadata-cache', action='store_true', help="Use metadata cache")
    parser.add_argument('--force', action='store', type=str, help="Force update the file followed by the --force argument")
    parser.add_argument('--path', action='store', type=str, help="Process only the file at this path")
    args = parser.parse_args()

    # Determine publishing platforms based on provided arguments
    # If none of the publish_xxx flags are provided, default to publishing on all platforms
    if not any([args.pub_xhs, args.pub_bilibili, args.pub_douyin, args.pub_y2b, args.pub_shipinhao]):
        publish_xhs = publish_bilibili = publish_douyin = publish_y2b = publish_shipinhao = True
    else:
        publish_xhs = args.pub_xhs
        publish_bilibili = args.pub_bilibili
        publish_douyin = args.pub_douyin
        publish_shipinhao = args.pub_shipinhao
        publish_y2b = args.pub_y2b

    if args.no_pub:
        publish_xhs = False
        publish_bilibili = False
        publish_douyin = False
        publish_shipinhao = False
        publish_y2b = False



    # Determine publishing platforms based on provided arguments
    # publish_xhs = args.pub_xhs
    # publish_bilibili = args.pub_bilibili
    # publish_douyin = args.pub_douyin
    test_mode = args.test
    use_cache = args.use_cache
    force_filename = args.force
    if not (force_filename is None):
        force_filename = force_filename.strip()
    else:
        force_filename = ""
    force_files = force_filename.split(",")

    use_translation_cache = args.use_translation_cache
    use_metadata_cache = args.use_metadata_cache



    current_datetime = datetime.now()
    log_filename = f"{current_datetime.strftime('%Y-%m-%d %H-%M-%S')}.txt"
    log_file_path = os.path.join(logs_folder_path, log_filename)

    # Define video file pattern
    video_file_pattern = re.compile(r'.+\.(mp4|mov|avi|flv|wmv|mkv)$', re.IGNORECASE)
    

    # Single file mode
    if args.path:
        filename = os.path.basename(args.path)
        if video_file_pattern.match(filename):
            processed_files = read_csv(processed_path)
            if filename not in processed_files or force_filename:
                print("process and publish file: ", args.path)
                process_and_publish_file(
                    args.path,
                    publish_xhs=publish_xhs,
                    publish_bilibili=publish_bilibili,
                    publish_douyin=publish_douyin,
                    publish_y2b=publish_y2b,
                    publish_shipinhao=publish_shipinhao,
                    test_mode=test_mode,
                    use_cache=use_cache,
                    use_translation_cache=use_translation_cache,
                    use_metadata_cache=use_metadata_cache
                )
                update_csv_if_new(filename, processed_path)
        else:
            print(f"The file {filename} does not match the video file pattern or has already been processed.")
    else:
        # Check each file in the autopublish folder
        for filename in os.listdir(autopublish_folder_path):
            if filename.startswith("preprocessed"):
                update_csv_if_new(filename, processed_path)
                continue

            if video_file_pattern.match(filename):
                file_path = os.path.join(autopublish_folder_path, filename)
                if os.path.isfile(file_path):
                    # Check and update videos_db.csv
                    update_csv_if_new(filename, videos_db_path)
                    
                    processed_files = read_csv(processed_path)
                    # If not processed, process the file and update processed.csv
                    # if (filename not in processed_files) or (force_filename != "" and force_filename in filename):
                    # if (force_filename and force_filename in filename) or (not force_filename and filename not in processed_files):
                    if ( (force_files and any(force_file.strip() in filename for force_file in force_files)) or ( filename and filename in force_files)) or (not force_filename and filename not in processed_files):
                        print("process and publish file: ", file_path)
                        process_and_publish_file(
                            file_path,
                            publish_xhs=publish_xhs,
                            publish_bilibili=publish_bilibili,
                            publish_douyin=publish_douyin,
                            publish_y2b=publish_y2b,
                            publish_shipinhao=publish_shipinhao,
                            test_mode=test_mode,
                            use_cache=use_cache,
                            use_translation_cache=use_translation_cache,
                            use_metadata_cache=use_metadata_cache
                        )
                        update_csv_if_new(filename, processed_path)

# After all tasks are done, remove the lock file
if os.path.exists(lock_file_path):
    os.remove(lock_file_path)

# Re-execute the Bash script
# print("Re-executing the Bash script...")
# subprocess.run(["bash", bash_script_path])


===== ./process_queue.sh =====
#!/bin/bash

# Define variables
QUEUE_LIST="/home/lachlan/Projects/autopub_monitor/queue_list.txt"
LOG_DIR="/home/lachlan/Projects/autopub_monitor/logs-autopub"
PUBLISH_SCRIPT="/home/lachlan/Projects/autopub_monitor/autopub.sh"
QUEUE_LOCK="/home/lachlan/Projects/autopub_monitor/queue.lock"

# Function to echo with timestamp
echo_with_timestamp() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1"
}

# Ensure the log directory and list file exist
mkdir -p "${LOG_DIR}"
touch "${QUEUE_LIST}"
touch "${QUEUE_LOCK}"
echo_with_timestamp "Starting process_queue.sh script..."

# Main loop to process files from the queue
echo_with_timestamp "Entering main processing loop..."
while true; do
    TIMESTAMP=$(date +%s)
    TMP_FILE="/tmp/queue_path_$TIMESTAMP.txt"
    {
        flock -x 200
        if [ -s "$QUEUE_LIST" ]; then
            full_path=$(head -n 1 "$QUEUE_LIST")
            echo "$full_path" > "$TMP_FILE"
            echo_with_timestamp "Read from queue inside lock: $full_path"
        else
            echo "" > "$TMP_FILE"
        fi
    } 200>"$QUEUE_LOCK"

    full_path=$(cat "$TMP_FILE")
    echo_with_timestamp "Variable full_path after lock: $full_path"
    
    if [ -n "$full_path" ]; then
        echo_with_timestamp "Processing: ${full_path}"
        bash "$PUBLISH_SCRIPT" "${full_path}" &>> "${LOG_DIR}/autopub.log"
        result=$?
        if [ $result -eq 0 ]; then
            echo_with_timestamp "Processing completed for: ${full_path}"
            {
                flock -x 200
                sed -i '1d' "$QUEUE_LIST"
                echo_with_timestamp "Removed from queue: $full_path"
            } 200>"$QUEUE_LOCK"
        else
            echo_with_timestamp "Processing failed for: ${full_path} with error code $result"
        fi
        
        rm "$TMP_FILE"
    else
        # echo_with_timestamp "No valid file to process. Waiting for new files in the queue..."
        sleep 1
    fi

done


===== ./requeue.sh =====
#!/bin/bash

# Directory to observe
OBSERVE_DIR="/home/lachlan/AutoPublishDATA/AutoPublish"
QUEUE_PIPE="/home/lachlan/Projects/autopub_monitor/queue.pipe"

# Confirmation flag
SKIP_CONFIRMATION=false

# Helper function to queue file
queue_file() {
    local file_path=$1
    echo "$file_path" > "$QUEUE_PIPE" &
    echo "Queued: $file_path"
}

# Process flags
while getopts "y" opt; do
    case $opt in
        y) SKIP_CONFIRMATION=true ;;
        \?) echo "Invalid option: -$OPTARG" >&2; exit 1 ;;
    esac
done
shift $((OPTIND -1))

# Check if any IDs are provided
if [ $# -eq 0 ]; then
    echo "Usage: $0 [-y] pattern_or_full_path"
    exit 1
fi

# Support full paths or patterns
input="$1"
if [[ "$input" == *"/"* ]]; then
    # If the input contains a slash, treat it as a full path
    if [ -f "$input" ]; then
        queue_file "$input"
    else
        echo "File does not exist: $input"
        exit 1
    fi
else
    # Use find to handle patterns and special characters
    mapfile -t matches < <(find "$OBSERVE_DIR" -type f -iname "*$input*")

    # Check for no matches
    if [ ${#matches[@]} -eq 0 ]; then
        echo "No files matched."
        exit 0
    fi

    # Function to handle file selection and queuing
    handle_queueing() {
        if [ "$SKIP_CONFIRMATION" = true ] || [ ${#matches[@]} -eq 1 ]; then
            queue_file "${matches[0]}"
        else
            echo "Multiple files matched. Please select the file(s) to queue (comma-separated numbers):"
            local i=1
            for f in "${matches[@]}"; do
                echo "$i) $f"
                ((i++))
            done
            read -p "#? " selection
            IFS=',' read -r -a selections <<< "$selection"
            for sel in "${selections[@]}"; do
                # Validate selection
                if [[ "$sel" =~ ^[0-9]+$ ]] && [ "$sel" -ge 1 ] && [ "$sel" -le ${#matches[@]} ]; then
                    queue_file "${matches[$sel-1]}"
                else
                    echo "Invalid selection: $sel"
                fi
            done
        fi
    }

    # Queue files based on confirmation requirement
    handle_queueing
fi

# Background any ongoing jobs
disown -a



===== ./autopub.sh =====
#!/bin/bash

# Load the user's bash profile to ensure all environment variables are set
source ~/.bashrc  # or source ~/.profile if you're using bash

# Activate Conda environment
source /home/lachlan/miniconda3/bin/activate autopub-video

# Capture the first argument as the full path
full_path="$1"

# Function to echo with timestamp
echo_with_timestamp() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1"
}

echo_with_timestamp "Executing autopub.py with file: ${full_path}..."

# Define the lock file and log file
lock_file="/home/lachlan/ProjectsLFS/autopub_monitor/autopub.lock"
log_dir="/home/lachlan/ProjectsLFS/autopub_monitor/logs-autopub"
log_file="${log_dir}/autopub_$(date '+%Y-%m-%d_%H-%M-%S').log"

# Create log directory if it doesn't exist
mkdir -p "${log_dir}"

# Wait for lock file to be released
while [ -f "${lock_file}" ]; do
    echo_with_timestamp "Another instance of the script is running. Waiting..."
    sleep 10  # Adjusted to check every 10 seconds
done

# Create a lock file
touch "${lock_file}"

# Ensure the lock file is removed when the script finishes
trap 'rm -f "${lock_file}"; exit' INT TERM EXIT

echo_with_timestamp "Executing autopub.py..."
if [ -n "${full_path}" ]; then
    # If a full path is provided, run the script with the --path argument
    echo_with_timestamp "Processing file: ${full_path}..."
    sleep 10
    /home/lachlan/miniconda3/envs/autopub-video/bin/python /home/lachlan/ProjectsLFS/autopub_monitor/autopub.py --use-cache --use-metadata-cache --use-translation-cache --path "${full_path}" > "${log_file}" 2>&1
else
    sleep 10
    # If no path is provided, run the script without the --path argument
    /home/lachlan/miniconda3/envs/autopub-video/bin/python /home/lachlan/ProjectsLFS/autopub_monitor/autopub.py --use-cache --use-metadata-cache --use-translation-cache > "${log_file}" 2>&1
fi

echo_with_timestamp "Finished executing autopub.py with file: ${full_path}..."

# Remove the lock file and clear the trap
rm -f "${lock_file}"
trap - INT TERM EXIT

echo_with_timestamp "Finished autopub.sh..."


===== ./queue_list.txt =====


===== ./process_queue.sh.old.sh =====
#!/bin/bash

# Define variables
QUEUE_PIPE="/home/lachlan/Projects/autopub_monitor/queue.pipe"
LIST_FILE="/home/lachlan/Projects/autopub_monitor/queue_list.txt"
LOG_DIR="/home/lachlan/Projects/autopub_monitor/logs-autopub"
PUBLISH_SCRIPT="/home/lachlan/Projects/autopub_monitor/autopub.sh"

# Function to echo with timestamp
echo_with_timestamp() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1"
}

# Ensure the log directory and list file exist
mkdir -p "${LOG_DIR}"
touch "${LIST_FILE}"
echo_with_timestamp "Starting process_queue.sh script..."

# Main loop to process files from the queue
echo_with_timestamp "Entering main processing loop..."
while true; do
    if read -r full_path; then
        echo_with_timestamp "Received path: ${full_path}"

        # Add path to the list file for tracking
        echo "${full_path}" >> "${LIST_FILE}"
        echo_with_timestamp "Updated list with new path."

        # Display the current queue list
        echo_with_timestamp "Current queue list:"
        cat "${LIST_FILE}"

        # Process the file
        echo_with_timestamp "Processing: ${full_path}"
        bash "$PUBLISH_SCRIPT" "${full_path}"
        echo_with_timestamp "Processing completed for: ${full_path}"

        # Remove the processed path from the list file
        grep -vxF "${full_path}" "${LIST_FILE}" > "${LIST_FILE}.tmp" && mv "${LIST_FILE}.tmp" "${LIST_FILE}"
        echo_with_timestamp "Removed processed path from list."

        # Display the updated queue list
        echo_with_timestamp "Updated queue list:"
        cat "${LIST_FILE}"
    else
    	sleep 1
        echo_with_timestamp "Waiting for input from the queue..."
    fi
done < "${QUEUE_PIPE}"


===== ./README.md =====
# AutoPubMonitor
This is the file monitoring and synchronisation  system for our AutoPublication system. 


===== ./autopub_monitor_tmux_session.sh =====
#!/bin/bash

export HOME_DIR=~

if [ "$1" = "start" ]; then
    # Create the 'autopub-sync' session and execute the rsync command within it
    if tmux has-session -t video-sync 2>/dev/null; then
        echo "Session video-sync already exists."
    else
        tmux new-session -d -s video-sync
        # tmux send-keys -t video-sync "cd $HOME_DIR && rsync -avh --progress $HOME_DIR/jianguoyun/AutoPublishDATA/ $HOME_DIR/AutoPublishDATA/" C-m
        # tmux send-keys -t video-sync "while true; do rsync -avh --delete --progress $HOME_DIR/jianguoyun/AutoPublishDATA/AutoPublish/ $HOME_DIR/AutoPublishDATA/AutoPublish/; sleep 10; done" C-m
        # tmux send-keys -t video-sync "while true; do rsync -avh --progress $HOME_DIR/jianguoyun/AutoPublishDATA/AutoPublish/ $HOME_DIR/AutoPublishDATA/AutoPublish/; sleep 10; done" C-m
        # tmux send-keys -t video-sync "while true; do rsync -rtvvv --progress /home/lachlan/jianguoyun/AutoPublishDATA/AutoPublish/ /home/lachlan/AutoPublishDATA/AutoPublish/; sleep 10; done" C-m
        # tmux send-keys -t video-sync "while true; do rsync -rt --progress --whole-file /home/lachlan/jianguoyun/AutoPublishDATA/AutoPublish/ /home/lachlan/AutoPublishDATA/AutoPublish/; sleep 10; done" C-m
        tmux send-keys -t video-sync "cd /home/lachlan/Projects/autopub_monitor/" C-m
        tmux send-keys -t video-sync "clear" C-m
        tmux send-keys -t video-sync "bash /home/lachlan/Projects/autopub_monitor/autopub_sync.sh" C-m
    fi

    # Start or ensure the monitor-autopub session is running
    if ! tmux has-session -t monitor-autopub 2>/dev/null; then
        tmux new-session -d -s monitor-autopub -c /home/lachlan/AutoPublishDATA/AutoPublish
        tmux send-keys -t monitor-autopub "cd /home/lachlan/AutoPublishDATA/AutoPublish" C-m
        tmux send-keys -t monitor-autopub "clear" C-m
        tmux send-keys -t monitor-autopub "/home/lachlan/ProjectsLFS/autopub_monitor/monitor_autopublish.sh" C-m
    fi

    # Start or ensure the process-queue session is running
    if ! tmux has-session -t process-queue 2>/dev/null; then
        tmux new-session -d -s process-queue -c /home/lachlan/ProjectsLFS/autopub_monitor
        tmux send-keys -t process-queue "cd /home/lachlan/ProjectsLFS/autopub_monitor" C-m
        tmux send-keys -t process-queue "clear" C-m
        tmux send-keys -t process-queue "./process_queue.sh" C-m
    fi

    
    # Create the 'transcription-sync' session for syncing transcription_data directory
    if ! tmux has-session -t transcription-sync 2>/dev/null; then
        tmux new-session -d -s transcription-sync
        tmux send-keys -t transcription-sync "while true; do rsync -avh --progress $HOME_DIR/AutoPublishDATA/transcription_data/ $HOME_DIR/jianguoyun/AutoPublishDATA/transcription_data/; sleep 10; done"
    fi
elif [ "$1" = "stop" ]; then
    # Stop the monitor-autopub and process-queue tmux sessions
    tmux kill-session -t video-sync 2>/dev/null
    tmux kill-session -t monitor-autopub 2>/dev/null
    tmux kill-session -t process-queue 2>/dev/null
    tmux kill-session -t transcription-sync 2>/dev/null
else
    echo "Usage: $0 {start|stop}"
    exit 1
fi



===== ./autopub_sync.sh =====
#!/bin/bash

src="/home/lachlan/jianguoyun/AutoPublishDATA/AutoPublish/"
dst="/home/lachlan/AutoPublishDATA/AutoPublish/"

while true; do
    # Function to check if the filename contains a date in any recognizable format
    contains_date() {
        if [[ $1 =~ [0-9]{4}-[0-9]{2}-[0-9]{2} ]] || [[ $1 =~ VID_[0-9]{4}[0-9]{2}[0-9]{2}_[0-9]{6} ]] || [[ $1 =~ [0-9]{4}_[0-9]{2}_[0-9]{2}_[0-9]{2}_[0-9]{2}_[0-9]{2} ]]; then
            return 0 # True, contains a date
        else
            return 1 # False, does not contain a date
        fi
    }

    # Check for non-zero file size, then rename files using modification time
    process_file() {
        local src_file=$1
        local file_size=$(stat --format="%s" "$src_file")
        
        if [[ "$file_size" -le 0 ]]; then
            echo "File size of $src_file is 0, waiting for transfer to complete..."
            sleep 5
        else
            local filename=$(basename "$src_file")
            local extension="${filename##*.}"
            local base="${filename%.*}"
            local suffix="_COMPLETED"
            local mod_time=$(stat --format="%y" "$src_file" | cut -d'.' -f1 | tr ' :-' '_')
            
            # Check if "_COMPLETED" suffix is already present
            if [[ $filename != *"$suffix"* ]]; then
                if contains_date "$filename"; then
                    new_filename="${base}${suffix}.${extension}"
                else
                    new_filename="${base}_${mod_time}${suffix}.${extension}"
                fi
                
                local new_path=$(dirname "$src_file")/"$new_filename"
                mv "$src_file" "$new_path"
                echo "Renamed $src_file to $new_path"
            else
                # echo "$filename already has the $suffix suffix."
                :
            fi
        fi
    }

    # Process files ensuring they have a non-zero file size
    find "$src" -type f -size +0c | while read src_file; do
        process_file "$src_file"
    done

    # Perform the rsync operation, including only files with the _COMPLETED suffix
    rsync -rt --progress --delete --whole-file --min-size=1 --include="*_COMPLETED.*" --exclude="*" "$src" "$dst"
    
    # Wait before repeating the operation
    sleep 10
done


===== ./test_xdo.py =====
import subprocess

def get_current_window_name():
    try:
        # Get the ID of the active window
        active_window_id = subprocess.check_output(["xdotool", "getactivewindow"]).decode().strip()

        # Get the name of the active window using its ID
        window_name = subprocess.check_output(["xdotool", "getwindowname", active_window_id]).decode().strip()

        return window_name
    except subprocess.CalledProcessError as e:
        print(f"Error: {e.output.decode()}")
        return None

if __name__ == "__main__":
    window_name = get_current_window_name()
    if window_name:
        print(f"The name of the current active window is: '{window_name}'")
    else:
        print("Failed to retrieve the active window name.")



===== ./monitor_autopublish.sh =====
#!/bin/bash

# Paths and initial setup
PUBLISH_SCRIPT="/home/lachlan/Projects/autopub_monitor/autopub.sh"
DIRECTORY_TO_OBSERVE="/home/lachlan/AutoPublishDATA/AutoPublish"
QUEUE_LIST="/home/lachlan/Projects/autopub_monitor/queue_list.txt"
TEMP_QUEUE="/home/lachlan/Projects/autopub_monitor/temp_queue.txt"
CHECKED_LIST="/home/lachlan/Projects/autopub_monitor/checked_list.txt"
QUEUE_LOCK="/home/lachlan/Projects/autopub_monitor/queue.lock"

echo_with_timestamp() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1"
}

echo_with_timestamp "Watching directory: $DIRECTORY_TO_OBSERVE for new files or files moved here."
touch "${QUEUE_LIST}"
touch "${TEMP_QUEUE}"
touch "${CHECKED_LIST}"
touch "${QUEUE_LOCK}"

check_and_queue_file() {
    local full_path=$1

    # Check if file has been checked and found invalid before
    if grep -Fxq "$full_path" "${CHECKED_LIST}"; then
        echo_with_timestamp "File $full_path has been checked and is invalid. Skipping."
        return
    fi

    local file_size=$(stat -c %s "$full_path")

    if [ "$file_size" -eq 0 ] || ! ffprobe -v error -show_entries format=filename -of default=noprint_wrappers=1:nokey=1 "$full_path" > /dev/null; then
        sleep 3 # Wait before moving to TEMP_QUEUE
        handle_potential_conflict_file "$full_path"
    else
        queue_file "$full_path"
    fi
}

queue_file() {
    local file_path=$1
    local sleep_time=$(( RANDOM % 30 + 1 ))  # Random sleep between 1 and 30 seconds
    sleep $sleep_time
    echo_with_timestamp "File $file_path passed checks after a random sleep of $sleep_time seconds. Adding to queue."

    # Lock the queue list, write the file path, and unlock
    (
        flock -x 200
        echo "$file_path" >> "$QUEUE_LIST"
    ) 200>"$QUEUE_LOCK"
}

handle_potential_conflict_file() {
    local original_file_path=$1
    local directory=$(dirname -- "$original_file_path")
    local base_name=$(basename -- "$original_file_path")
    local name_without_ext="${base_name%.*}"
    local prefix=$(echo "$name_without_ext" | sed -E 's/_[0-9]{4}_[0-9]{2}_[0-9]{2}_[0-9]{2}_[0-9]{2}_[0-9]{2}$//')

    # Search for conflict files, considering variable timestamp and NSConflict marker
    for file in "$directory"/*; do
        if [[ "$file" =~ ${prefix}-NSConflict-.* ]] && ffprobe -v error -show_entries format=filename -of default=noprint_wrappers=1:nokey=1 "$file" > /dev/null; then
            echo_with_timestamp "Valid conflict version found: $file. Original file $original_file_path will be skipped from further processing."
            # Mark original file as checked (invalid)
            echo "$original_file_path" >> "${CHECKED_LIST}"
            return
        fi
    done

    # No valid conflict found, and original file is invalid, move it to TEMP_QUEUE
    echo_with_timestamp "No valid conflict found. Original file $original_file_path is invalid. Moving to TEMP_QUEUE."
    echo "$original_file_path" >> "$TEMP_QUEUE"
}


monitor_temp_queue() {
    while true; do
        if [ ! -s "$TEMP_QUEUE" ]; then
            sleep 5
            continue
        fi

        cp "$TEMP_QUEUE" "${TEMP_QUEUE}_copy"
        > "$TEMP_QUEUE"

        while IFS= read -r line; do
            if [ -f "$line" ]; then
                check_and_queue_file "$line"
            fi
        done < "${TEMP_QUEUE}_copy"
        rm "${TEMP_QUEUE}_copy"
    done
}

monitor_temp_queue &

inotifywait -m -e close_write -e moved_to "$DIRECTORY_TO_OBSERVE" |
while read -r directory events filename; do
    if [[ "$filename" =~ ^\..*\..*\..*$ ]]; then
        echo_with_timestamp "Skipping temporary or system file: $filename"
        continue
    fi

    full_path="${directory}${filename}"
    echo_with_timestamp "Significant change detected: $full_path"
    check_and_queue_file "$full_path"
done


===== ./checked_list.txt =====
/home/lachlan/AutoPublishDATA/AutoPublish/C0025_2024_03_04_05_52_32.MP4


===== ./ignore_list.txt =====


===== ./process_video.py =====
import os
import requests
from urllib.parse import urlparse
from pathlib import Path
from requests_toolbelt import MultipartEncoder

import subprocess

import os
import tempfile


def get_video_length(filename):
    """Returns the length of the video in seconds or None if unable to determine."""
    try:
        cmd = f"ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 \"{filename}\""
        output = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT)
        video_length = float(output)
        return video_length
    except Exception as e:
        print(f"Warning: Failed to get video length for {filename}. Error: {e}")
        return None


# def augment_video(video_path, augmented_length, output_path):
#     """
#     Repeats the video to ensure it reaches at least the specified minimum length.
#     If the video already meets or exceeds the minimum length, no repetition is performed.

#     Args:
#         video_path (str): Path to the input video.
#         augmented_length (int): Minimum desired length of the video in seconds.
#         output_path (str): Path to the output augmented video.
#     """
#     # Get the length of the input video
#     video_length = get_video_length(video_path)  # Assume this function is already defined

#     # # Check if repetition is needed
#     # if video_length >= augmented_length:
#     #     print(f"No repetition needed. Video length ({video_length}s) already meets or exceeds the minimum length ({augmented_length}s).")
#     #     if video_path != output_path:
#     #         # Copy the original video to the output path if they are not the same
#     #         shutil.copy(video_path, output_path)
#     #     return output_path

#     # Calculate how many times the video needs to be repeated
#     repeat_count = int(augmented_length / video_length) + (augmented_length % video_length > 0)

#     # Generate a temporary file listing for ffmpeg
#     concat_file_path = "concat_list.txt"
#     with open(concat_file_path, "w") as file:
#         for _ in range(repeat_count):
#             file.write(f"file '{video_path}'\n")

#     # Use ffmpeg to concatenate the video repeats
#     ffmpeg_command = [
#         "ffmpeg", '-y', "-f", "concat", "-safe", "0", "-i", concat_file_path,
#         "-c", "copy", output_path
#     ]
#     subprocess.run(ffmpeg_command, check=True)

#     # Clean up temporary file
#     os.remove(concat_file_path)

#     return output_path

def augment_video(video_path, augmented_length, output_path):
    """
    Repeats the video to ensure it reaches at least the specified minimum length.
    If the video already meets or exceeds the minimum length, no repetition is performed.

    Args:
        video_path (str): Path to the input video.
        augmented_length (int): Minimum desired length of the video in seconds.
        output_path (str): Path to the output augmented video.
    """
    try:
        # Assume get_video_length is already defined and correctly implemented
        video_length = get_video_length(video_path)  # Implement this function to get the video length

        if video_length >= augmented_length:
            print(f"No augmentation needed. Video length ({video_length}s) already meets or exceeds the minimum length ({augmented_length}s).")
            if video_path != output_path:
                # Copy the original video to the output path if they are not the same
                shutil.copy(video_path, output_path)
            return output_path

        repeat_count = int(augmented_length / video_length) + (augmented_length % video_length > 0)
        print(f"Repeating the video {repeat_count} times to meet the minimum length requirement.")

        # Generate a temporary file listing for ffmpeg
        concat_file_path = "concat_list.txt"
        with open(concat_file_path, "w") as file:
            for _ in range(repeat_count):
                file.write(f"file '{video_path}'\n")

        # Update ffmpeg command to re-encode audio for MP4 compatibility
        ffmpeg_command = [
            "ffmpeg", '-y', "-f", "concat", "-safe", "0", "-i", concat_file_path,
            "-c:v", "copy", "-c:a", "aac", "-b:a", "192k", output_path
        ]
        print(f"Executing FFmpeg command: {' '.join(ffmpeg_command)}")
        subprocess.run(ffmpeg_command, check=True)

        print(f"Video successfully augmented and saved to {output_path}")
    except subprocess.CalledProcessError as e:
        print(f"Error during video augmentation: {e}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
    finally:
        # Clean up temporary file
        if os.path.exists(concat_file_path):
            os.remove(concat_file_path)

    return output_path

class VideoProcessor:



    def __init__(self, upload_url, process_url, video_path, transcription_path):
        self.upload_url = upload_url
        self.process_url = process_url
        self.video_path = video_path
        self.transcription_path = transcription_path
        os.makedirs(self.transcription_path, exist_ok=True)



        input_file = self.video_path
        # Define the minimum length for the video in seconds
        augmented_length = 7  # 7 seconds
        threshold_length = augmented_length

        # Attempt to check the video length
        video_length = get_video_length(input_file)
        
        print("video length: ", video_length)

        # Skip augmentation if the video length is greater than augmented_length or if it couldn't be determined
        if video_length is None or video_length > threshold_length:
        # if False:
            if video_length is None:
                print(f"Warning: Could not determine video length for {input_file}. Skipping augmentation.")
            else:
                print(f"Video is longer than {augmented_length} seconds, skipping augmentation.")
            
        else:
            # Proceed with augmentation if the video is shorter than the augmented_length
            # Ensure augment_video_if_needed is properly defined to handle the logic
            print(f"Video length {video_length} is shorter than {threshold_length}. Augmented to {augmented_length}. ")

            # input_file = yield self.augment_video_if_needed(input_file, augmented_length)
            input_file = self.augment_video_if_needed(input_file, augmented_length)

        self.video_path = input_file

    # @run_on_executor
    def augment_video_if_needed(self, input_file, augmented_length):
        print("input_file: ", input_file)

        # base_name, extension = os.path.splitext(os.path.basename(input_file))
        # output_folder = os.path.dirname(input_file)
        # augmented_video_path = os.path.join(output_folder, f"{base_name}_augmented_{augmented_length}s{extension}")

        base_name, extension = os.path.splitext(os.path.basename(input_file))
        # Using tempfile to create a temporary directory
        temp_dir = tempfile.mkdtemp()
        augmented_video_path = os.path.join(temp_dir, f"{base_name}_augmented_{augmented_length}s{extension}")

        print("augmented_video_length: ", augmented_video_path)

        # Perform the augmentation
        new_path = augment_video(input_file, augmented_length, augmented_video_path)
        return new_path

    def process_video(self, 
        use_cache=False,
        use_translation_cache=False,
        use_metadata_cache=False
    ):



        video_name = Path(self.video_path).stem
        zip_file_root = os.path.join(self.transcription_path, video_name)
        os.makedirs(zip_file_root, exist_ok=True)
        zip_file_path = os.path.join(zip_file_root, f"{video_name}.zip")

        # Check cache
        if use_cache and os.path.isfile(zip_file_path):
            print(f"Cache hit! Returning the processed file from {zip_file_path}.")
            return zip_file_path

        else:
            if not os.path.isfile(zip_file_path):
                print(f"{zip_file_path} not found. ")
                print("Cache miss. Uploading video for processing.")
            else:
                print("Cache ignored: use_cache=false.")
            

        
        if not self.upload_url.endswith("stream"):
            with open(self.video_path, 'rb') as f:
                files = {'video': (os.path.basename(self.video_path), f)}
                response = requests.post(
                    self.upload_url, 
                    files=files, 
                    data={'filename': os.path.basename(self.video_path)}
                )
        else:
            # Preprocess the file for streaming upload
            preprocessed_file_path = self.preprocess_for_streaming(self.video_path)
            with open(preprocessed_file_path, 'rb') as f:
                files = {'video': (os.path.basename(preprocessed_file_path), f)}
                response = requests.put(
                    self.upload_url, 
                    files=files, 
                    params={'filename': os.path.basename(preprocessed_file_path)}
                )
 


        if not response.ok:
            print(f'Failed to upload file. Status code: {response.status_code}, Message: {response.text}')
            return

        # Extract the file path from the response
        uploaded_file_path = response.json().get('file_path')
        if not uploaded_file_path:
            print("Failed to get the uploaded file path from the server response.")
            return

        # Request processing of the uploaded file
        process_response = requests.post(
            self.process_url, 
            data={
                'file_path': uploaded_file_path, 
                "use_translation_cache": use_translation_cache,
                "use_metadata_cache": use_metadata_cache
            }
        )
        if process_response.ok:
            with open(zip_file_path, 'wb') as f:
                f.write(process_response.content)
            print(f'Success! Processed files are downloaded and saved to {zip_file_path}.')
            return zip_file_path
        else:
            print(f'Failed to process file. Status code: {process_response.status_code}, Message: {process_response.text}')
    
    def preprocess_for_streaming(self, file_path):
        output_file_path = os.path.join(os.path.dirname(file_path), 'preprocessed_' + os.path.basename(file_path))
        # Explicitly specify the video and audio codec along with copying the streams and moving the moov atom
        command = f"ffmpeg -y -i \"{file_path}\" -vcodec copy -acodec copy -movflags faststart \"{output_file_path}\""
        try:
            subprocess.run(command, shell=True, check=True)
            print(f"Successfully preprocessed {file_path} to {output_file_path}")
        except subprocess.CalledProcessError as e:
            print(f"Failed to preprocess file with FFmpeg: {e}")
            return file_path  # Return original file path in case of failure
        return output_file_path



if __name__ == "__main__":
    # Usage
    video_path = '/Users/lachlan/Nutstore Files/Vlog/AutoPublish/IMG_5304.MOV'
    server_url = 'http://lachlanserver:8081/video-processing'
    transcription_path = "/Users/lachlan/Nutstore Files/Vlog/transcription_data"

    processor = VideoProcessor(server_url, video_path, transcription_path)
    processor.process_video()


===== ./temp_queue.txt =====


